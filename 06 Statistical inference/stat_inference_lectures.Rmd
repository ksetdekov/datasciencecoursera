---
title: "Statistic inference lecture"
author: "Kirill Setdekov"
date: "March 10, 2019"
output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
"Generating conclusions about a populations from a noisy sample"

*pick frequentist paradigm.*
[link to main repository](https://github.com/ksetdekov/courses/tree/master/06_StatisticalInference)

```{r libload, message=FALSE, include=FALSE}
#require(swirl)
#install_from_swirl("Statistical Inference")
```

## Probability

$$P(A\cup B)=P(A)+P(B)$$

$$ P(A \cup B)=P(A)+P(B)-P(A \cap B)$$

## probability mass function and density function

* PMF
    * must be larger or equal to 0
    * the sum of possible values sum up to 1
* PDF 
    * must be larger or equal to 0
    * the sum of possible values sum up to 1. Area under PDFs correspond to probabilities for that random variables

    
```{r}
x <- c(-0.5,0,1,1,1.5)
y <- c(0,0,2,0,0)
plot(x,y, lwd=3, frame = FALSE, type="l")


```

## CDF and survival function

- The **cumulative distribution function** (CDF) of a random variable $X$ is defined as the function 
$$
F(x) = P(X \leq x)
$$
- This definition applies regardless of whether $X$ is discrete or continuous.
- The **survival function** of a random variable $X$ is defined as
$$
S(x) = P(X > x)
$$
- Notice that $S(x) = 1 - F(x)$
- For continuous random variables, the PDF is the derivative of the CDF


$$
S(x) = 1 - x^2
$$

```{r}
pbeta(c(0.4, 0.5, 0.6), 2, 1)
```

## Quantiles

You've heard of sample quantiles. If you were the 95th percentile on an exam, you know
that 95% of people scored worse than you and 5% scored better. 
These are sample quantities. Here we define their population analogs.



## Definition

The  $\alpha^{th}$ **quantile** of a distribution with distribution function $F$ is the point $x_\alpha$ so that
$$
F(x_\alpha) = \alpha
$$
- A **percentile** is simply a quantile with $\alpha$ expressed as a percent
- The **median** is the $50^{th}$ percentile



## Example
What is the median of the distribution that we were working with before?
- We want to solve $0.5 = F(x) = x^2$
- Resulting in the solution 
```{r, echo = TRUE} 
sqrt(0.5)
``` 
- Therefore, about `r sqrt(0.5)` of calls being answered on a random day is the median.


## Example continued
R can approximate quantiles for you for common distributions

```{r}
qbeta(0.5, 2, 1) #quantiles
```


## summary
- We're referring to are **population quantities**. Therefore, the median being
  discussed is the **population median**.
- A probability model connects the data to the population using assumptions.
- Therefore the median we're discussing is the **estimand**, the sample median will be the **estimator**


# conditinal probabilities
## Conditional probability, definition

- Let $B$ be an event so that $P(B) > 0$
- Then the conditional probability of an event $A$ given that $B$ has occurred is
  $$
  P(A ~|~ B) = \frac{P(A \cap B)}{P(B)}
  $$
- Notice that if $A$ and $B$ are independent (defined later in the lecture), then
  $$
  P(A ~|~ B) = \frac{P(A) P(B)}{P(B)} = P(A)
  $$
 
 ## bayes' rule
 
 ## Bayes' rule
Baye's rule allows us to reverse the conditioning set provided
that we know some marginal probabilities
$$
P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}.
$$

## Diagnostic tests

- Let $+$ and $-$ be the events that the result of a diagnostic test is positive or negative respectively
- Let $D$ and $D^c$ be the event that the subject of the test has or does not have the disease respectively 
- The **sensitivity** is the probability that the test is positive given that the subject actually has the disease, $P(+ ~|~ D)$
- The **specificity** is the probability that the test is negative given that the subject does not have the disease, $P(- ~|~ D^c)$

---

## More definitions

- The **positive predictive value** is the probability that the subject has the  disease given that the test is positive, $P(D ~|~ +)$
- The **negative predictive value** is the probability that the subject does not have the disease given that the test is negative, $P(D^c ~|~ -)$
- The **prevalence of the disease** is the marginal probability of disease, $P(D)$

---

## More definitions

- The **diagnostic likelihood ratio of a positive test**, labeled $DLR_+$, is $P(+ ~|~ D) / P(+ ~|~ D^c)$, which is the $$sensitivity / (1 - specificity)$$
- The **diagnostic likelihood ratio of a negative test**, labeled $DLR_-$, is $P(- ~|~ D) / P(- ~|~ D^c)$, which is the $$(1 - sensitivity) / specificity$$

## Independence

- Two events $A$ and $B$ are **independent** if $$P(A \cap B) = P(A)P(B)$$
- Equivalently if $P(A ~|~ B) = P(A)$ 
- Two random variables, $X$ and $Y$ are independent if for any two sets $A$ and $B$ $$P([X \in A] \cap [Y \in B]) = P(X\in A)P(Y\in B)$$
- If $A$ is independent of $B$ then 
  - $A^c$ is independent of $B$ 
  - $A$ is independent of $B^c$
  - $A^c$ is independent of $B^c$
  
## The population mean
- The **expected value** or **mean** of a random variable is the center of its distribution
- For discrete random variable $X$ with PMF $p(x)$, it is defined as follows
    $$
    E[X] = \sum_x xp(x).
    $$
    where the sum is taken over the possible values of $x$
- $E[X]$ represents the center of mass of a collection of locations and weights, $\{x, p(x)\}$

---
## The sample mean
- The sample mean estimates this population mean
- The center of mass of the data is the empirical mean
$$
\bar X = \sum_{i=1}^n x_i p(x_i)
$$

## simulation example
Distribution of averages
```{r, fig.height=6, figh.width=6, fig.align='center', echo = FALSE}
library(ggplot2)
nosim <- 10000
n <- 10
dat <- data.frame(x = c(rnorm(nosim), apply(matrix(
rnorm(nosim * n), nosim
), 1, mean)),
what = factor(rep(c("Obs", "Mean"), c(nosim, nosim))))
ggplot(dat, aes(x = x, fill = what)) + geom_density(size = 2, alpha = .2)

```

## Averages of x die rolls

The population mean of averages of two die rolls is exactly the same as the population mean of die rolls.

## Sumarizing what we know
- Expected values are properties of distributions
- The population mean is the center of mass of population
- The sample mean is the center of mass of the observed data
- The sample mean is an estimate of the population mean
- The sample mean is unbiased 
  - The population mean of its distribution is the mean that it's
  trying to estimate
- The more data that goes into the sample mean, the more 
concentrated its density / mass function is around the population mean

```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}  
dat <- data.frame(
  x = c(sample(1 : 6, nosim, replace = TRUE),
        apply(matrix(sample(1 : 6, nosim * 2, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 3, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 4, replace = TRUE), 
                     nosim), 1, mean)
        ),
  size = factor(rep(1 : 4, rep(nosim, 4))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.25, colour = "black") 
g + facet_grid(. ~ size)
```

## Averages of x coin flips
```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}
dat <- data.frame(
  x = c(sample(0 : 1, nosim, replace = TRUE),
        apply(matrix(sample(0 : 1, nosim * 10, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(0 : 1, nosim * 20, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(0 : 1, nosim * 100, replace = TRUE), 
                     nosim), 1, mean)
        ),
  size = factor(rep(c(1, 10, 20, 100), rep(nosim, 4))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20,  colour = "black"); 
g + facet_grid(. ~ size)
```
R has *integration* 

integrate(mypdf,0,1.6)

## Variablility

## The variance

- The variance of a random variable is a measure of *spread*
- If $X$ is a random variable with mean $\mu$, the variance of $X$ is defined as

$$
Var(X) = E[(X - \mu)^2] = E[X^2] - E[X]^2
$$ 

## Example

- What's the variance from the result of the toss of a coin with probability of heads (1) of $p$? 

  - $E[X] = 0 \times (1 - p) + 1 \times p = p$
  - $E[X^2] = E[X] = p$ 

$$Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1 - p)$$


## Distributions with increasing variance
```{r, echo = FALSE, fig.height = 6, fig.width = 8, fig.align='center'}
library(ggplot2)
xvals <- seq(-10, 10, by = .01)
dat <- data.frame(
    y = c(
        dnorm(xvals, mean = 0, sd = 1),
        dnorm(xvals, mean = 0, sd = 2),
        dnorm(xvals, mean = 0, sd = 3),
        dnorm(xvals, mean = 0, sd = 4)
    ),
    x = rep(xvals, 4),
    factor = factor(rep(1 : 4, rep(length(xvals), 4)))
)
ggplot(dat, aes(x = x, y = y, color = factor)) + geom_line(size = 2)    
```

Sample variance 
- The sample variance is 
$$
S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}
$$

## variability
```{r norm}
nosim <- 10000
n <- 10
sd(apply(matrix(rnorm(nosim*n),nosim), 1, mean))
1/sqrt(n)
```
```{r poison}
nosim <- 10000
n <- 10
sd(apply(matrix(rpois(nosim*n,4),nosim), 1, mean))
2/sqrt(n)
```
```{r coin}
nosim <- 10000
n <- 10
sd(apply(matrix(sample(0:1,nosim*n, replace = T),nosim), 1, mean))
1/(2*sqrt(n))
```

Variance example
```{r}
library(UsingR)
data("father.son")
x <- father.son$sheight
n <- length(x)
qplot(x, geom = 'blank') +   
  geom_line(aes(y = ..density.., colour = 'Empirical'), stat = 'density') +  
  geom_histogram(aes(y = ..density..), alpha = 0.4) +                        
  theme(legend.position = c(0.85, 0.85))
round(c(var(x),var(x)/n,sd(x),sd(x)/sqrt(n)),2)
```

## distributions
$$ nCx=(n!)/(x!\cdot(n-x)!) $$
$$ nC0=nCx=1 $$
Probability to have 7 or more girs of 8 births

```{r girls}
choose(8,7)*.5^8+choose(8,8)*.5^8
pbinom(6, size = 8, prob = .5, lower.tail = F)
```

## normal distribution
```{r normal}
x <- seq(-4, 4, length=100)
hx <- dnorm(x)
ggplot(data.frame(cbind(x, hx))) + 
    geom_line(aes(x = x, y = hx)) + 
    geom_vline(xintercept =qnorm(.95, 0, 1),
               linetype = "dotted",
               color = "blue")
                                                                                  


```

## poisson example
Mean people per hour = 2.4. Probability of 3 or fiewer in 4 hours is:
```{r}
ppois(3,lambda = 2.5*4)
```

## poisson approx for binomial
flip a coin with success of 0.01
probability of 2 or fewer successes?
```{r poissapprox}
pbinom(2, size = 500, prob = 0.01)
ppois(2,lambda = 500*0.01)

x <- 1:500
require(dplyr)
require(reshape)
x <- data.frame(x)
x <- x %>% mutate(binom = pbinom(2, size = x, prob = 0.01),
             pois = ppois(2,lambda = x*0.01))
x_long <- reshape::melt(x, id.vars = "x")
ggplot(x_long)+geom_line(aes(x=x,  y=value, colour = variable))
```
# Asymptotics

``` {r}
n <- 1000
means <- cumsum(rnorm(n))/(1:n)
plot(means)

## coinflips
means <- cumsum(sample(0:1, n, replace = TRUE))/(1:n)
plot(means)
```

We define estimator as consistent if it converges to what you mant to estimate.
LLN says that sample mean of iid sample is sonsistent for the population mean.
Sample variance and sample SD are consistent as well.

# Central Limit theorem
distribution of averages of iid variables becomes that of a standard normal as the sample size increases.
- The CLT applies in an endless variety of settings
- Let $X_1,\ldots,X_n$ be a collection of iid random variables with mean $\mu$ and variance $\sigma^2$
- Let $\bar X_n$ be their sample average
- Then $\frac{\bar X_n - \mu}{\sigma / \sqrt{n}}$ has a distribution like that of a standard normal for large $n$.
- Remember the form
$$\frac{\bar X_n - \mu}{\sigma / \sqrt{n}} = 
    \frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}.
$$
- Usually, replacing the standard error by its estimated value doesn't change the CLT

## Coin CLT

 - Let $X_i$ be the $0$ or $1$ result of the $i^{th}$ flip of a possibly unfair coin
- The sample proportion, say $\hat p$, is the average of the coin flips
- $E[X_i] = p$ and $Var(X_i) = p(1-p)$
- Standard error of the mean is $\sqrt{p(1-p)/n}$
- Then
$$
    \frac{\hat p - p}{\sqrt{p(1-p)/n}}
$$
will be approximately normally distributed

---

```{r, echo = FALSE, fig.width=7.5, fig.height = 5}
par(mfrow = c(2, 3))
for (n in c(1, 10, 20)){
  temp <- matrix(sample(0 : 1, n * 10000, replace = TRUE), ncol = n)
  temp <- apply(temp, 1, mean)
  temp <- (temp - .5) * 2 * sqrt(n)
  dty <- density(temp)
  plot(dty$x, dty$y, xlab = "", ylab = "density", type = "n", xlim = c(-3, 3), ylim = c(0, .5))
  title(paste("sample mean of", n, "obs"))
  lines(seq(-3, 3, length = 100), dnorm(seq(-3, 3, length = 100)), col = grey(.8), lwd = 3)
  lines(dty$x, dty$y, lwd = 2)
}
for (n in c(1, 10, 20)){
  temp <- matrix(sample(0 : 1, n * 10000, replace = TRUE, prob = c(.9, .1)), ncol = n)
  temp <- apply(temp, 1, mean)
  temp <- (temp - .1) / sqrt(.1 * .9 / n)
  dty <- density(temp)
  plot(dty$x, dty$y, xlab = "", ylab = "density", type = "n", xlim = c(-3, 3), ylim = c(0, .5))
  title(paste("sample mean of", n, "obs"))
  lines(seq(-3, 3, length = 100), dnorm(seq(-3, 3, length = 100)), col = grey(.8), lwd = 3)
  lines(dty$x, dty$y, lwd = 2)
}
```
