---
title: "Statistic inference lecture"
author: "Kirill Setdekov"
date: "March 10, 2019"
output:
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
"Generating conclusions about a populations from a noisy sample"

*pick frequentist paradigm.*
[link to main repository](https://github.com/ksetdekov/courses/tree/master/06_StatisticalInference)

```{r libload, message=FALSE, include=FALSE}
#require(swirl)
#install_from_swirl("Statistical Inference")
```

## Probability

$$P(A\cup B)=P(A)+P(B)$$

$$ P(A \cup B)=P(A)+P(B)-P(A \cap B)$$

## probability mass function and density function

* PMF
    * must be larger or equal to 0
    * the sum of possible values sum up to 1
* PDF 
    * must be larger or equal to 0
    * the sum of possible values sum up to 1. Area under PDFs correspond to probabilities for that random variables

    
```{r}
x <- c(-0.5,0,1,1,1.5)
y <- c(0,0,2,0,0)
plot(x,y, lwd=3, frame = FALSE, type="l")


```

## CDF and survival function

- The **cumulative distribution function** (CDF) of a random variable $X$ is defined as the function 
$$
F(x) = P(X \leq x)
$$
- This definition applies regardless of whether $X$ is discrete or continuous.
- The **survival function** of a random variable $X$ is defined as
$$
S(x) = P(X > x)
$$
- Notice that $S(x) = 1 - F(x)$
- For continuous random variables, the PDF is the derivative of the CDF


$$
S(x) = 1 - x^2
$$

```{r}
pbeta(c(0.4, 0.5, 0.6), 2, 1)
```

## Quantiles

You've heard of sample quantiles. If you were the 95th percentile on an exam, you know
that 95% of people scored worse than you and 5% scored better. 
These are sample quantities. Here we define their population analogs.



## Definition

The  $\alpha^{th}$ **quantile** of a distribution with distribution function $F$ is the point $x_\alpha$ so that
$$
F(x_\alpha) = \alpha
$$
- A **percentile** is simply a quantile with $\alpha$ expressed as a percent
- The **median** is the $50^{th}$ percentile



## Example
What is the median of the distribution that we were working with before?
- We want to solve $0.5 = F(x) = x^2$
- Resulting in the solution 
```{r, echo = TRUE} 
sqrt(0.5)
``` 
- Therefore, about `r sqrt(0.5)` of calls being answered on a random day is the median.


## Example continued
R can approximate quantiles for you for common distributions

```{r}
qbeta(0.5, 2, 1) #quantiles
```


## summary
- We're referring to are **population quantities**. Therefore, the median being
  discussed is the **population median**.
- A probability model connects the data to the population using assumptions.
- Therefore the median we're discussing is the **estimand**, the sample median will be the **estimator**


# conditinal probabilities
## Conditional probability, definition

- Let $B$ be an event so that $P(B) > 0$
- Then the conditional probability of an event $A$ given that $B$ has occurred is
  $$
  P(A ~|~ B) = \frac{P(A \cap B)}{P(B)}
  $$
- Notice that if $A$ and $B$ are independent (defined later in the lecture), then
  $$
  P(A ~|~ B) = \frac{P(A) P(B)}{P(B)} = P(A)
  $$
 
 ## bayes' rule
 
 ## Bayes' rule
Baye's rule allows us to reverse the conditioning set provided
that we know some marginal probabilities
$$
P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}.
$$

## Diagnostic tests

- Let $+$ and $-$ be the events that the result of a diagnostic test is positive or negative respectively
- Let $D$ and $D^c$ be the event that the subject of the test has or does not have the disease respectively 
- The **sensitivity** is the probability that the test is positive given that the subject actually has the disease, $P(+ ~|~ D)$
- The **specificity** is the probability that the test is negative given that the subject does not have the disease, $P(- ~|~ D^c)$

---

## More definitions

- The **positive predictive value** is the probability that the subject has the  disease given that the test is positive, $P(D ~|~ +)$
- The **negative predictive value** is the probability that the subject does not have the disease given that the test is negative, $P(D^c ~|~ -)$
- The **prevalence of the disease** is the marginal probability of disease, $P(D)$

---

## More definitions

- The **diagnostic likelihood ratio of a positive test**, labeled $DLR_+$, is $P(+ ~|~ D) / P(+ ~|~ D^c)$, which is the $$sensitivity / (1 - specificity)$$
- The **diagnostic likelihood ratio of a negative test**, labeled $DLR_-$, is $P(- ~|~ D) / P(- ~|~ D^c)$, which is the $$(1 - sensitivity) / specificity$$

## Independence

- Two events $A$ and $B$ are **independent** if $$P(A \cap B) = P(A)P(B)$$
- Equivalently if $P(A ~|~ B) = P(A)$ 
- Two random variables, $X$ and $Y$ are independent if for any two sets $A$ and $B$ $$P([X \in A] \cap [Y \in B]) = P(X\in A)P(Y\in B)$$
- If $A$ is independent of $B$ then 
  - $A^c$ is independent of $B$ 
  - $A$ is independent of $B^c$
  - $A^c$ is independent of $B^c$
  
## The population mean
- The **expected value** or **mean** of a random variable is the center of its distribution
- For discrete random variable $X$ with PMF $p(x)$, it is defined as follows
    $$
    E[X] = \sum_x xp(x).
    $$
    where the sum is taken over the possible values of $x$
- $E[X]$ represents the center of mass of a collection of locations and weights, $\{x, p(x)\}$

---
## The sample mean
- The sample mean estimates this population mean
- The center of mass of the data is the empirical mean
$$
\bar X = \sum_{i=1}^n x_i p(x_i)
$$

## simulation example
Distribution of averages
```{r, fig.height=6, figh.width=6, fig.align='center', echo = FALSE}
library(ggplot2)
nosim <- 10000
n <- 10
dat <- data.frame(x = c(rnorm(nosim), apply(matrix(
rnorm(nosim * n), nosim
), 1, mean)),
what = factor(rep(c("Obs", "Mean"), c(nosim, nosim))))
ggplot(dat, aes(x = x, fill = what)) + geom_density(size = 2, alpha = .2)

```

## Averages of x die rolls

The population mean of averages of two die rolls is exactly the same as the population mean of die rolls.

## Sumarizing what we know
- Expected values are properties of distributions
- The population mean is the center of mass of population
- The sample mean is the center of mass of the observed data
- The sample mean is an estimate of the population mean
- The sample mean is unbiased 
  - The population mean of its distribution is the mean that it's
  trying to estimate
- The more data that goes into the sample mean, the more 
concentrated its density / mass function is around the population mean

```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}  
dat <- data.frame(
  x = c(sample(1 : 6, nosim, replace = TRUE),
        apply(matrix(sample(1 : 6, nosim * 2, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 3, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 4, replace = TRUE), 
                     nosim), 1, mean)
        ),
  size = factor(rep(1 : 4, rep(nosim, 4))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.25, colour = "black") 
g + facet_grid(. ~ size)
```

## Averages of x coin flips
```{r, fig.align='center',fig.height=5, fig.width=10, echo = FALSE, warning=FALSE, error=FALSE, message=FALSE}
dat <- data.frame(
  x = c(sample(0 : 1, nosim, replace = TRUE),
        apply(matrix(sample(0 : 1, nosim * 10, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(0 : 1, nosim * 20, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(0 : 1, nosim * 100, replace = TRUE), 
                     nosim), 1, mean)
        ),
  size = factor(rep(c(1, 10, 20, 100), rep(nosim, 4))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20,  colour = "black"); 
g + facet_grid(. ~ size)
```
R has *integration* 

integrate(mypdf,0,1.6)

## Variablility

## The variance

- The variance of a random variable is a measure of *spread*
- If $X$ is a random variable with mean $\mu$, the variance of $X$ is defined as

$$
Var(X) = E[(X - \mu)^2] = E[X^2] - E[X]^2
$$ 

## Example

- What's the variance from the result of the toss of a coin with probability of heads (1) of $p$? 

  - $E[X] = 0 \times (1 - p) + 1 \times p = p$
  - $E[X^2] = E[X] = p$ 

$$Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1 - p)$$


## Distributions with increasing variance
```{r, echo = FALSE, fig.height = 6, fig.width = 8, fig.align='center'}
library(ggplot2)
xvals <- seq(-10, 10, by = .01)
dat <- data.frame(
    y = c(
        dnorm(xvals, mean = 0, sd = 1),
        dnorm(xvals, mean = 0, sd = 2),
        dnorm(xvals, mean = 0, sd = 3),
        dnorm(xvals, mean = 0, sd = 4)
    ),
    x = rep(xvals, 4),
    factor = factor(rep(1 : 4, rep(length(xvals), 4)))
)
ggplot(dat, aes(x = x, y = y, color = factor)) + geom_line(size = 2)    
```

Sample variance 
- The sample variance is 
$$
S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1}
$$

## variability
```{r norm}
nosim <- 10000
n <- 10
sd(apply(matrix(rnorm(nosim*n),nosim), 1, mean))
1/sqrt(n)
```
```{r poison}
nosim <- 10000
n <- 10
sd(apply(matrix(rpois(nosim*n,4),nosim), 1, mean))
2/sqrt(n)
```
```{r coin}
nosim <- 10000
n <- 10
sd(apply(matrix(sample(0:1,nosim*n, replace = T),nosim), 1, mean))
1/(2*sqrt(n))
```

Variance example
```{r}
library(UsingR)
data("father.son")
x <- father.son$sheight
n <- length(x)
qplot(x, geom = 'blank') +   
  geom_line(aes(y = ..density.., colour = 'Empirical'), stat = 'density') +  
  geom_histogram(aes(y = ..density..), alpha = 0.4) +                        
  theme(legend.position = c(0.85, 0.85))
round(c(var(x),var(x)/n,sd(x),sd(x)/sqrt(n)),2)
```

## distributions
$$ nCx=(n!)/(x!\cdot(n-x)!) $$
$$ nC0=nCx=1 $$
Probability to have 7 or more girs of 8 births

```{r girls}
choose(8,7)*.5^8+choose(8,8)*.5^8
pbinom(6, size = 8, prob = .5, lower.tail = F)
```

## normal distribution
```{r normal}
x <- seq(-4, 4, length=100)
hx <- dnorm(x)
ggplot(data.frame(cbind(x, hx))) + 
    geom_line(aes(x = x, y = hx)) + 
    geom_vline(xintercept =qnorm(.95, 0, 1),
               linetype = "dotted",
               color = "blue")
                                                                                  


```

## poisson example
Mean people per hour = 2.4. Probability of 3 or fiewer in 4 hours is:
```{r}
ppois(3,lambda = 2.5*4)
```

## poisson approx for binomial
flip a coin with success of 0.01
probability of 2 or fewer successes?
```{r poissapprox}
pbinom(2, size = 500, prob = 0.01)
ppois(2,lambda = 500*0.01)

x <- 1:500
require(dplyr)
require(reshape)
x <- data.frame(x)
x <- x %>% mutate(binom = pbinom(2, size = x, prob = 0.01),
             pois = ppois(2,lambda = x*0.01))
x_long <- reshape::melt(x, id.vars = "x")
ggplot(x_long)+geom_line(aes(x=x,  y=value, colour = variable))
```
# Asymptotics

``` {r}
n <- 1000
means <- cumsum(rnorm(n))/(1:n)
plot(means)

## coinflips
means <- cumsum(sample(0:1, n, replace = TRUE))/(1:n)
plot(means)
```

We define estimator as consistent if it converges to what you mant to estimate.
LLN says that sample mean of iid sample is sonsistent for the population mean.
Sample variance and sample SD are consistent as well.

# Central Limit theorem
distribution of averages of iid variables becomes that of a standard normal as the sample size increases.
- The CLT applies in an endless variety of settings
- Let $X_1,\ldots,X_n$ be a collection of iid random variables with mean $\mu$ and variance $\sigma^2$
- Let $\bar X_n$ be their sample average
- Then $\frac{\bar X_n - \mu}{\sigma / \sqrt{n}}$ has a distribution like that of a standard normal for large $n$.
- Remember the form
$$\frac{\bar X_n - \mu}{\sigma / \sqrt{n}} = 
    \frac{\mbox{Estimate} - \mbox{Mean of estimate}}{\mbox{Std. Err. of estimate}}.$$

- Usually, replacing the standard error by its estimated value doesn't change the CLT

## Coin CLT

 - Let $X_i$ be the $0$ or $1$ result of the $i^{th}$ flip of a possibly unfair coin
- The sample proportion, say $\hat p$, is the average of the coin flips
- $E[X_i] = p$ and $Var(X_i) = p(1-p)$
- Standard error of the mean is $\sqrt{p(1-p)/n}$
- Then

    $$\frac{\hat p - p}{\sqrt{p(1-p)/n}}$$
will be approximately normally distributed




```{r, echo = FALSE, fig.width=7.5, fig.height = 5}
par(mfrow = c(2, 3))
for (n in c(1, 10, 20)){
  temp <- matrix(sample(0 : 1, n * 10000, replace = TRUE), ncol = n)
  temp <- apply(temp, 1, mean)
  temp <- (temp - .5) * 2 * sqrt(n)
  dty <- density(temp)
  plot(dty$x, dty$y, xlab = "", ylab = "density", type = "n", xlim = c(-3, 3), ylim = c(0, .5))
  title(paste("sample mean of", n, "obs"))
  lines(seq(-3, 3, length = 100), dnorm(seq(-3, 3, length = 100)), col = grey(.8), lwd = 3)
  lines(dty$x, dty$y, lwd = 2)
}
for (n in c(1, 10, 1000)){
  temp <- matrix(sample(0 : 1, n * 10000, replace = TRUE, prob = c(.9, .1)), ncol = n)
  temp <- apply(temp, 1, mean)
  temp <- (temp - .1) / sqrt(.1 * .9 / n)
  dty <- density(temp)
  plot(dty$x, dty$y, xlab = "", ylab = "density", type = "n", xlim = c(-3, 3), ylim = c(0, .5))
  title(paste("sample mean of", n, "obs"))
  lines(seq(-3, 3, length = 100), dnorm(seq(-3, 3, length = 100)), col = grey(.8), lwd = 3)
  lines(dty$x, dty$y, lwd = 2)
}
```


# coinflip examples
| Suppose we were going to flip a biased coin 5 times. The probability of
| tossing a head is .8 and a tail .2. What is the probability that you'll toss
| at least 3 heads.

```{r}
i <- 0
for (x in 3:5) {
     i <- i+choose(5,x)*(.8)^x*(.2)^(5-x)
}
i

pbinom(2,size=5,prob=.8,lower.tail=FALSE)
```

# про распределения
```{r}
pnorm(1200,mean=1020,sd=50,lower.tail=FALSE) #lower tail false - gives p above
pnorm((1200-1020)/50,lower.tail=FALSE)

#percentile
qnorm(0.75, mean = 1020, sd = 50, lower.tail = TRUE)
```

## poisson
$$ X \sim Poisson(lambda \cdot t)$$
suppose the number of people that show up at a bus stop is Poisson with a mean of
2.5 per hour, and we want to know the probability that at most 3 people show up 
in a 4 hour period.
```{r}
ppois(3,2.5*4)
#compare poisson and binomial
pbinom(5,1000,.01)
ppois(5,1000*.01)
```

## Give a confidence interval for the average height of sons
in Galton's data
```{r}
library(UsingR);data(father.son); x <- father.son$sheight
(mean(x) + c(-1, 1) * qnorm(.975) * sd(x) / sqrt(length(x))) / 12
```
## Sample proportions

- In the event that each $X_i$ is $0$ or $1$ with common success probability $p$ then $\sigma^2 = p(1 - p)$
- The interval takes the form
$$ \hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{p(1 - p)}{n}}$$
- Replacing $p$ by $\hat p$ in the standard error results in what is called a Wald confidence interval for $p$
- Also note that $p(1-p) \leq 1/4$ for $0 \leq p \leq 1$
- Let $\alpha = .05$ so that $z_{1 -\alpha/2} = 1.96 \approx 2$ then
$$
    2  \sqrt{\frac{p(1 - p)}{n}} \leq 2 \sqrt{\frac{1}{4n}} = \frac{1}{\sqrt{n}} 
$$
- Therefore $\hat p \pm \frac{1}{\sqrt{n}}$ is a quick CI estimate for $p$

---
## Example
* Your campaign advisor told you that in a random sample of 100 likely voters,
  56 intent to vote for you. 
  * Can you relax? Do you have this race in the bag?
  * Without access to a computer or calculator, how precise is this estimate?
* `1/sqrt(100)=.1` so a back of the envelope calculation gives an approximate 95% interval of `(0.46, 0.66)`
  * Not enough for you to relax, better go do more campaigning!
* Rough guidelines, 100 for 1 decimal place, 10,000 for 2, 1,000,000 for 3.
```{r}
round(1 / sqrt(10 ^ (1 : 6)), 3)
```

### R code
```{r}
0.56+c(-1,1)*qnorm(0.975)*sqrt(0.56*0.44/100)
binom.test(56,100)
```

# consider a simulation - bootstrap test

```{r}
n <- 20
pval <- seq(0.1,0.9, by=0.05)
nosim <- 1000
coverage <- sapply(pval, function(p){
 phat <- rbinom(nosim, prob = p, size = n)/n
 lowerlimit <- phat-qnorm(0.975)*sqrt(phat*(1-phat)/n)
 upperlimit <- phat+qnorm(0.975)*sqrt(phat*(1-phat)/n)
 mean(lowerlimit<p&upperlimit>p)
})
qplot(pval,coverage, geom = "line")+geom_hline(yintercept=0.95,linetype="dashed", color = "red")
```

quick fix - form the interval with Agresti/Coull interval. Add 2 successes and faulures
$$\frac{X+2}{n+4}$$
```{r}
n <- 20
pval <- seq(0.1,0.9, by=0.05)
nosim <- 1000
coverage <- sapply(pval, function(p){
 phat <- (rbinom(nosim, prob = p, size = n)+2)/(n+4)
 lowerlimit <- phat-qnorm(0.975)*sqrt(phat*(1-phat)/n)
 upperlimit <- phat+qnorm(0.975)*sqrt(phat*(1-phat)/n)
 mean(lowerlimit<p&upperlimit>p)
})
qplot(pval,coverage, geom = "line")+geom_hline(yintercept=0.95,linetype="dashed", color = "red")
```

## Poisson interval
* A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?
* $X \sim Poisson(\lambda t)$.
* Estimate $\hat \lambda = X/t$
* $Var(\hat \lambda) = \lambda / t$ 

$$\frac{\hat \lambda - \lambda}{\sqrt{\hat \lambda / t}} = 
\frac{X - t \lambda}{\sqrt{X}} 
\rightarrow N(0,1)$$
* This isn't the best interval.
  * There are better asymptotic intervals.
  * You can get an exact CI in this case.
  
 ### R code
```{r}
x <- 5; t <- 94.32; lambda <- x / t
round(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)
poisson.test(x, T = 94.32)$conf
```

# simulatea a poisson coverage
```{r poiscoverage}
labdavals <- seq(0.005, 0.1, by = 0.01)
nosim <- 1000
t <- 10000
coverage <- sapply(labdavals, function(lambda) {
    lhat <- rpois(nosim, lambda = lambda * t) / t
    lowerlimit <- lhat - qnorm(0.975) * sqrt(lhat / t)
    upperlimit <- lhat + qnorm(0.975) * sqrt(lhat / t)
    mean(lowerlimit < lambda & upperlimit > lambda)
})
qplot(labdavals,coverage, geom = "line")+geom_hline(yintercept=0.95,linetype="dashed", color = "red")
```



## In the regression class
```{r}
exp(confint(glm(x ~ 1 + offset(log(t)), family = poisson(link = log))))
```

# summary
* lln states that averages of iid sample converges to the population means that they are estimating.
* the CLT states that averages are approximatesly normal with distributions
    * centerd at population mean
    * $SD$ equal to standard error of the mean
    * CLT gives no guarantee that $n$ is large enough
    
* Poisso and binomial case have exact intervals that do not require the CLT
    * but a quick fix for small sample size for binomial is to add to fails and 2 successes

for the test
You flip a fair coin 5 times, about what's the probability of getting 4 or 5 heads?
```{r}
choose(5,5)*.5^5+choose(5,4)*.5^5
```
The respiratory disturbance index (RDI), a measure of sleep disturbance, for a specific population has a mean of 15 (sleep events per hour) and a standard deviation of 10. They are not normally distributed. Give your best estimate of the probability that a sample mean RDI of 100 people is between 14 and 16 events per hour?
```{r}
pnorm(16, mean = 15, sd = 10/sqrt(100)) - pnorm(14, mean = 15, sd = 10/sqrt(100))
```
