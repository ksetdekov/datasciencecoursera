---
title: "Regression models lectures 3"
author: "Kirill Setdekov"
date: "August 7, 2019"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	cache = TRUE
)
```

# week 3

## Multivariable  regression analyses.

If lots of predictors, not one.

The linear model - linear in coefficients
$$Y_i = \sum^p_{k=1}{X_{ik}\beta_j}+\epsilon_i$$

```{r example01}
n = 100
x <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
y <- 1 + x + x2 + x3 + rnorm(n, sd = .1)
ey <- resid(lm(y ~ x2 + x3))
ex <- resid(lm(x ~ x2 + x3))
sum(ey * ex) / sum(ex ^ 2)
coef(lm(ey ~ ex - 1))
coef(lm(y ~ x + x2 + x3))
```

## Fitted values, residuals and residual variation
All of our SLR quantities can be extended to linear models

* Model $Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}$ where $\epsilon_i \sim N(0, \sigma^2)$

* Fitted responses $\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}$

* Residuals $e_i = Y_i - \hat Y_i$

* Variance estimate $\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2$

* To get predicted responses at new values, $x_1, \ldots, x_p$, simply plug them into the linear model $\sum_{k=1}^p x_{k} \hat \beta_{k}$

* Coefficients have standard errors, $\hat \sigma_{\hat \beta_k}$, and
$\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}$
follows a $T$ distribution with $n-p$ degrees of freedom.

* Predicted responses have standard errors and we can calculate predicted and expected response intervals.

```{r examples}
require(datasets)
data("swiss")
require(GGally)
require(ggplot2)
ggpairs(swiss, lower = list(continuous = wrap("smooth", method = "loess")))

#fitting all
summary(lm(Fertility~., data = swiss))
summary(lm(Fertility~., data = swiss))$coefficients

#only one result
summary(lm(Fertility~Agriculture, data = swiss))
```

How can adjustment reverse the sign of an effect? Let's try a simulation.
```{r, echo = TRUE}
n <- 100; x2 <- 1 : n; x1 <- .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01)
summary(lm(y ~ x1))$coef
summary(lm(y ~ x1 + x2))$coef
```

---
```{r changeinsign, echo = FALSE, fig.height=5, fig.width=10, results = 'show'}
par(mfrow = c(1, 2))
plot(
    x1,
    y,
    pch = 21,
    col = "black",
    bg = topo.colors(n)[x2],
    frame = FALSE,
    cex = 1.5
)
title('Unadjusted, color is X2')
abline(lm(y ~ x1), lwd = 2)
plot(
    resid(lm(x1 ~ x2)),
    resid(lm(y ~ x2)),
    pch = 21,
    col = "black",
    bg = "lightblue",
    frame = FALSE,
    cex = 1.5
)
title('Adjusted')
abline(0, coef(lm(y ~ x1 + x2))[2], lwd = 2)
```

## Dummy variables

More than 2 levels

* Consider a multilevel factor level. For didactic reasons, let's say a three level factor (example, US political party affiliation: Republican, Democrat, Independent)

* $Y_i = \beta_0 + X_{i1} \beta_1 + X_{i2} \beta_2 + \epsilon_i$.

* $X_{i1}$ is 1 for Republicans and 0 otherwise.

* $X_{i2}$ is 1 for Democrats and 0 otherwise.

* If $i$ is Republican $E[Y_i] = \beta_0 +\beta_1$

* If $i$ is Democrat $E[Y_i] = \beta_0 + \beta_2$.

* If $i$ is Independent $E[Y_i] = \beta_0$. 

* $\beta_1$ compares Republicans to Independents.

* $\beta_2$ compares Democrats to Independents.

* $\beta_1 - \beta_2$ compares Republicans to Democrats.

* (Choice of reference category changes the interpretation.)

```{r example02}
require(datasets)
data(InsectSprays)
require(stats)
require(graphics)
ggplot(data = InsectSprays, aes(y = count, x = spray, fill = spray)) +
    geom_violin(colour = "black", size = 1) +
    xlab("Type of spray") + ylab("insect count")
# model
summary(lm(count~spray, data = InsectSprays))
#without intercept
summary(lm(count~spray -1, data = InsectSprays))
```
Hardcode dummy - I are instances.
Reference level is "C"
```{r exapmle02hardcode}
summary(lm(count ~
               I(1 * (spray == 'A')) +
               I(1 * (spray == 'B')) +
               I(1 * (spray == 'D')) +
               I(1 * (spray == 'E')) +
               I(1 * (spray == 'F'))
           , data = InsectSprays))

```
Better way to relevel:

```{r exapmle02relevel}
spray2 <- relevel(InsectSprays$spray,"C")
summary(lm(count~spray2, data = InsectSprays))
```

Fitting multiple lines. ANCOVA
```{r example03}
require(datasets)
data("swiss")
hist(swiss$Catholic)
library(dplyr)
swiss <- swiss %>% mutate(CatholicBin = 1 * (Catholic > 50))

g <-
    ggplot(swiss, aes(
        x = Agriculture,
        y = Fertility,
        colour = factor(CatholicBin)
    )) +
    geom_point (size = 4, alpha = 0.75)

g
```

Could mace a factor - with x2 = 1 for CatholicBin

```{r example04simple}
fit <- lm(Fertility ~ Agriculture, data = swiss)
g1 <- g +
    geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2)
g1
summary(fit)
```
```{r example04withcatholic}
fit <-
    lm(Fertility ~ Agriculture + factor(CatholicBin), data = swiss)
g1 <- g +
    geom_abline(intercept = coef(fit)[1],
                slope = coef(fit)[2],
                size = 2)
g1 <- g1 +
    geom_abline(
        intercept = coef(fit)[1] + coef(fit)[3],
        slope = coef(fit)[2],
        size = 2
    )
g1
summary(fit)
```

With interaction
```{r example04interaction}
fit <-
    lm(Fertility ~ Agriculture * factor(CatholicBin), data = swiss) #fits all interaction
g1 <- g +
    geom_abline(intercept = coef(fit)[1],
                slope = coef(fit)[2],
                size = 2)
g1 <- g1 +
    geom_abline(
        intercept = coef(fit)[1] + coef(fit)[3],
        slope = coef(fit)[2] + coef(fit)[4],
        size = 2
    )
g1
summary(fit)
```

Adding adjustments

## Consider the following simulated data
Code for the first plot, rest omitted

```{r adjustmentsim}
n <-
    100
t <- rep(c(0, 1), c(n / 2, n / 2))
x <- c(runif(n / 2), runif(n / 2))

beta0 <- 0
beta1 <- 2
tau <- 1
sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1:(n / 2)]), lwd = 3)
abline(h = mean(y[(n / 2 + 1):n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(
    x[1:(n / 2)],
    y[1:(n / 2)],
    pch = 21,
    col = "black",
    bg = "lightblue",
    cex = 2
)
points(
    x[(n / 2 + 1):n],
    y[(n / 2 + 1):n],
    pch = 21,
    col = "black",
    bg = "salmon",
    cex = 2
)
```

## Simulation 2
strong marginal effect when disregard effect, very subtle effect if adjust for X
```{r adjustmentsim2, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
n <-
    100
t <-
    rep(c(0, 1), c(n / 2, n / 2))
x <- c(runif(n / 2), 1.5 + runif(n / 2))

beta0 <- 0
beta1 <- 2
tau <- 0
sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1:(n / 2)]), lwd = 3)
abline(h = mean(y[(n / 2 + 1):n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(
    x[1:(n / 2)],
    y[1:(n / 2)],
    pch = 21,
    col = "black",
    bg = "lightblue",
    cex = 2
)
points(
    x[(n / 2 + 1):n],
    y[(n / 2 + 1):n],
    pch = 21,
    col = "black",
    bg = "salmon",
    cex = 2
)
```

## Simulation 3 
illustration of simpson's paradox
```{r adjustmentsim3, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
n <-
    100
t <-
    rep(c(0, 1), c(n / 2, n / 2))
x <- c(runif(n / 2), .9 + runif(n / 2))

beta0 <- 0
beta1 <- 2
tau <- -1
sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1:(n / 2)]), lwd = 3)
abline(h = mean(y[(n / 2 + 1):n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(
    x[1:(n / 2)],
    y[1:(n / 2)],
    pch = 21,
    col = "black",
    bg = "lightblue",
    cex = 2
)
points(
    x[(n / 2 + 1):n],
    y[(n / 2 + 1):n],
    pch = 21,
    col = "black",
    bg = "salmon",
    cex = 2
)
```

## Simulation 4
no marginal effect, but a huge effect, when adjust for X
```{r adjustmentsim4, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
n <-
    100
t <-
    rep(c(0, 1), c(n / 2, n / 2))
x <- c(.5 + runif(n / 2), runif(n / 2))

beta0 <- 0
beta1 <- 2
tau <- 1
sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1:(n / 2)]), lwd = 3)
abline(h = mean(y[(n / 2 + 1):n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(
    x[1:(n / 2)],
    y[1:(n / 2)],
    pch = 21,
    col = "black",
    bg = "lightblue",
    cex = 2
)
points(
    x[(n / 2 + 1):n],
    y[(n / 2 + 1):n],
    pch = 21,
    col = "black",
    bg = "salmon",
    cex = 2
)
```

## Simulation 5
This will be wrong if we think that slopes are equeal - need to have an interaction term
```{r adjustmentsim5, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
n <-
    100
t <-
    rep(c(0, 1), c(n / 2, n / 2))
x <- c(runif(n / 2,-1, 1), runif(n / 2,-1, 1))

beta0 <- 0
beta1 <- 2
tau <- 0
tau1 <- -4
sigma <- .2
y <-
    beta0 + x * beta1 + t * tau + t * x * tau1 + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1:(n / 2)]), lwd = 3)
abline(h = mean(y[(n / 2 + 1):n]), lwd = 3)
fit <- lm(y ~ x + t + I(x * t))
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2] + coef(fit)[4], lwd = 3)
points(
    x[1:(n / 2)],
    y[1:(n / 2)],
    pch = 21,
    col = "black",
    bg = "lightblue",
    cex = 2
)
points(
    x[(n / 2 + 1):n],
    y[(n / 2 + 1):n],
    pch = 21,
    col = "black",
    bg = "salmon",
    cex = 2
)
```

### Simulation 6
Binary treatment - not necessary.

```{r adjustmentsim6, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
p <- 1
n <- 100
x2 <- runif(n)
x1 <- p * runif(n) - (1 - p) * x2
beta0 <- 0
beta1 <- 1
tau <- 4
sigma <- .01
y <- beta0 + x1 * beta1 + tau * x2 + rnorm(n, sd = sigma)
plot(x1, y, type = "n", frame = FALSE)
abline(lm(y ~ x1), lwd = 2)
co.pal <- heat.colors(n)
points(
    x1,
    y,
    pch = 21,
    col = "black",
    bg = co.pal[round((n - 1) * x2 + 1)],
    cex = 2
)
```

Need to look in 3d

```{r adjustmentsim63d}
library(rgl)
plot3d(x1, x2, y)
```
Better to look at residuals
Strong relationsip between Y and X2
```{r resuduals, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
plot(
    resid(lm(x1 ~ x2)),
    resid(lm(y ~ x2)),
    frame = FALSE,
    col = "black",
    bg = "lightblue",
    pch = 21,
    cex = 2
)
abline(lm(I(resid(lm(
    x1 ~ x2
))) ~ I(resid(lm(
    y ~ x2
)))), lwd = 2)
```

# residual diagnostics.
Linear model.
Define the residuals as

* $e_i = Y_i -  \hat Y_i =  Y_i - \sum_{k=1}^p X_{ik} \hat \beta_j$

* Our estimate of residual variation is $\hat \sigma^2 = \frac{\sum_{i=1}^n e_i^2}{n-p}$, the $n-p$ so that $E[\hat \sigma^2] = \sigma^2$

```{r residual plots, fig.height = 5, fig.width = 5}
data(swiss)
par(mfrow = c(2, 2))
fit <- lm(Fertility ~ . , data = swiss)
plot(fit)
```

Influential vs outlier points.
Leverage - how high away from the average is the point
Influence - how far away from the regression line.

## Influential, high leverage and outlying points
```{r influenceleverage, fig.height = 5, fig.width=5, echo = FALSE, results='hide'}
n <- 100
x <- rnorm(n)
y <- x + rnorm(n, sd = .3)
plot(
    c(-3, 6),
    c(-3, 6),
    type = "n",
    frame = FALSE,
    xlab = "X",
    ylab = "Y"
)
abline(lm(y ~ x), lwd = 2)
points(x, y, cex = 2, bg = "lightblue", col = "black", pch = 21)
points(0, 0, cex = 2, bg = "darkorange", col = "black", pch = 21)
points(0, 5, cex = 2, bg = "darkorange", col = "black", pch = 21)
points(5, 5, cex = 2, bg = "darkorange", col = "black", pch = 21)
points(5, 0, cex = 2, bg = "darkorange", col = "black", pch = 21)
text(0,0, labels = "low leverage,\nlow influence", pos = 4)
text(0,5, labels = "high leverage, high influence", pos = 1)
text(5,5, labels = "high levereage,\nlow influence", pos = 3)
```

* Outlier - too vague.
* Could be rear or spurious process.
* Can conform or not to the regression.

solutions - ```?influence.measures```

Main plot - residuals vs fitted values - if we have systematic patterns
Residual Q-Q plot - check normality
Leverage plot

## Influence measures
  * `rstandard` - standardized residuals, residuals divided by their standard deviations)
  * `rstudent` - standardized residuals, residuals divided by their standard deviations, where the ith data point was deleted in the calculation of the standard deviation for the residual to follow a t distribution
  * `hatvalues` - measures of leverage
  * `dffits` - change in the predicted response when the $i^{th}$ point is deleted in fitting the model.
  * `dfbetas` - change in individual coefficients when the $i^{th}$ point is deleted in fitting the model.
  * `cooks.distance` - overall change in the coefficients when the $i^{th}$ point is deleted.
  * `resid` - returns the ordinary residuals
  * `resid(fit) / (1 - hatvalues(fit))` where `fit` is the linear model fit returns the PRESS residuals, i.e. the leave one out cross validation residuals - the difference in the response and the predicted response at data point $i$, where it was not included in the model fitting.
  
# simulation experiments

## Case 1
```{r case1, fig.height=5, fig.width=5, echo=FALSE}
x <- c(10, rnorm(n)); y <- c(10, c(rnorm(n)))
plot(x, y, frame = FALSE, cex = 2, pch = 21, bg = "lightblue", col = "black")
abline(lm(y ~ x))   

par(mfrow = c(2, 2))
fit <- lm(y ~ x)
plot(fit)
```
```{r diagcase1}
fit <- lm(y ~ x)
round(dfbetas(fit)[1 : 10, 2], 3)
round(hatvalues(fit)[1 : 10], 3)
```
for point 1 there is a higher level of both metrics

## Case 2
```{r case2, fig.height=5, fig.width=5, echo=FALSE}
x <- rnorm(n)
y <- x + rnorm(n, sd = .3)
x <- c(5, x)
y <- c(5, y)
plot(
    x,
    y,
    frame = FALSE,
    cex = 2,
    pch = 21,
    bg = "lightblue",
    col = "black"
)
fit2 <- lm(y ~ x)
abline(fit2)  
```

```{r diagcase2, echo = TRUE}
round(dfbetas(fit2)[1 : 10, 2], 3)
round(hatvalues(fit2)[1 : 10], 3)
```

** Example by stefanski **
```{r stefanski, fig.height=4, fig.width=4}
require(GGally)
require(ggplot2)

dat <- read.table('http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt', header = FALSE)
pairs(dat)
ggpairs(dat, lower = list(continuous = wrap("smooth", method = "loess")))
```

## Got our P-values, should we bother to do a residual plot?
```{r}
summary(lm(V1 ~ . -1, data = dat))
```

## Residual plot - fin
### P-values significant, O RLY?
```{r residuals, fig.height=4, fig.width=4, echo = TRUE}
fit <- lm(V1 ~ . - 1, data = dat); plot(predict(fit), resid(fit), pch = '.')
```

## Back to the Swiss data
```{r, fig.height = 5, fig.width = 5, echo=FALSE}
data(swiss)
par(mfrow = c(2, 2))
fit <- lm(Fertility ~ . , data = swiss)
plot(fit)
```

* not many signs of heteroskedasticity
* looks normal
* third plots - scaled residuals
* residual vs scale - we dont want high leverage with high residuals