---
title: "Practical machine learning  lectures 3"
author: "Kirill Setdekov"
date: "December 02 2019"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	cache = TRUE
)
```

# week 3

## predicting with trees

1. start with all vars
2. find the best separating variable
3. divide the data into two leaves on that node
4. within each split, find the best variable/split that separates the outcomes
5. continue until the groups are too small or sufficientrly "pure"

### measures of impurity

Misclassification Error

$$\hat{P}_{mk} = \frac{1}{N_m}\sum_{x_i\; in \; Leaf \; m}\mathbb{1}(y_i = k)$$
__Misclassification Error__: 
$$ 1 - \hat{p}_{m k(m)}; k(m) = {\rm most; common; k}$$ 
* 0 = perfect purity
* 0.5 = no purity

__Gini index__:
$$ \sum_{k \neq k'} \hat{p}_{mk} \times \hat{p}_{mk'} = \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk}) = 1 - \sum_{k=1}^K p_{mk}^2$$

* 0 = perfect purity
* 0.5 = no purity

__Deviance/information gain__:

$$ -\sum_{k=1}^K \hat{p}_{mk} \log_2\hat{p}_{mk} $$
* 0 = perfect purity
* 1 = no purity

http://en.wikipedia.org/wiki/Decision_tree_learning

```{r example}
data(iris)
library(ggplot2)
require(caret)
names(iris)
table(iris$Species)
```

```{r trainingTest, dependson="iris",cache=TRUE}

inTrain <- createDataPartition(y = iris$Species,
                               p = 0.7,
                               list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
dim(training)
dim(testing)
```


---

## Iris petal widths/sepal width

```{r, dependson="trainingTest"}
qplot(Petal.Width, Sepal.Width, colour = Species, data = training)
```
```{r createTree, dependson="trainingTest", cache=TRUE}
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
```


## Plot tree

```{r, dependson="createTree"}
plot(modFit$finalModel, uniform=TRUE, 
      main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
```
## Prettier plots

```{r, dependson="createTree"}
library(rattle)
fancyRpartPlot(modFit$finalModel)
```

## Predicting new values

```{r newdata, dependson="createTree"}
predict(modFit,newdata=testing)
```


## Notes and further resources

* Classification trees are non-linear models
  * They use interactions between variables
  * Data transformations may be less important (monotone transformations)
  * Trees can also be used for regression problems (continuous outcome)
* Note that there are multiple tree building options
in R both in the caret package - [party](http://cran.r-project.org/web/packages/party/index.html), [rpart](http://cran.r-project.org/web/packages/rpart/index.html) and out of the caret package - [tree](http://cran.r-project.org/web/packages/tree/index.html)
* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Classification and regression trees](http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418)

## bootstrap aggregating (bagging)

1. reasmple causes and make predictions
2. average or majority vote for model

```{r}
library(ElemStatLearn)
data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone),]
head(ozone)
```

bagging example
```{r}
ll <- matrix(NA, nrow = 10, ncol = 155)
for (i in 1:10) {
    ss <- sample(1:dim(ozone)[1], replace = T)
    ozone0 <- ozone[ss, ]
    ozone0 <- ozone0[order(ozone0$ozone), ]
    loess0 <- loess(temperature ~ ozone, data = ozone0, span = 0.2)
    ll[i, ] <- predict(loess0, newdata = data.frame(ozone = 1:155))
}
#average
with(ozone, plot(ozone, temperature, pch = 19, cex = 0.5))
for (i in 1:10) {
    lines(1:155, ll[i,], col="grey", lwd=1)
}
lines(1:155, apply(ll,2,mean), col="red", lwd=2)
```


bagging - lower variability with same bias


## baggin in caret

Some models perform bagging in train with $method$ options

* bagEarth
* treebag
* bagFDA

## custom bagging

```{r}
predictors = data.frame(ozone = ozone$ozone)
temperature = ozone$temperature
library(caret)
treebag <-
    bag(
        predictors,
        temperature,
        B = 10,
        bagControl = bagControl(
            fit = ctreeBag$fit,
            predict = ctreeBag$pred,
            aggregate = ctreeBag$aggregate
        )
    )
```
### custom bagging examples
greqy - actual
red - one tree
blue - fit from bagged regression
```{r}
with(ozone, plot(ozone, temperature, col = "lightgrey", pch = 19, cex = 0.5))
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors), pch=19, col="red")
points(ozone$ozone, predict(treebag, predictors), pch=19, col="blue")
```
exploration
```{r}
ctreeBag$fit
ctreeBag$pred
ctreeBag$aggregate
```
__Notes__:

* Good for nonlinear model
* often used with trees - an extension is random forstrs
* some models use bagging in caret's train function

__Further resources__:

* [Bagging](http://en.wikipedia.org/wiki/Bootstrap_aggregating)
* [Bagging and boosting](http://stat.ethz.ch/education/semesters/FS_2008/CompStat/sk-ch8.pdf)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)

## random forest

1. bootstrap samples
2. each split, bootstrap variables
3. grow multiple trees and vote or average

pros:

1. accuracy

cons:

1. speed
2. interpretability
3. overfitting

```{r randomforest}
data("iris")
library(ggplot2)
library(caret)
library(randomForest)
inTrain <- createDataPartition(y = iris$Species, p =0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]

modFit <- train(Species~., data = training, method = "rf", prox = TRUE) #prox - extra info
modFit
```
```{r}
getTree(modFit$finalModel,k=2)
```
Class "centers" to show some of the model specifics - ceters for the predicted variables
```{r}
irisP <- classCenter(training[,c(3,4)] , training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP)
irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col = Species, data = training)
p+ geom_point(aes(x=Petal.Width, y= Petal.Length, col = Species), size = 5, shape = 4, data = irisP)
```

```{r rf_predict}

```

```{r}
pred <- predict(modFit, testing)
testing$predRight <- pred ==testing$Species
table(pred, testing$Species)

qplot(Petal.Width, Petal.Length, colour =predRight, data = testing, main = "newdata Predictions")
```

### Notes and further resources

__Notes__:

* Random forests are usually one of the two top
performing algorithms along with boosting in prediction contests.
* Random forests are difficult to interpret but often very accurate. 
* Care should be taken to avoid overfitting (see [rfcv](http://cran.r-project.org/web/packages/randomForest/randomForest.pdf) funtion)


__Further resources__:

* [Random forests](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)
* [Random forest Wikipedia](http://en.wikipedia.org/wiki/Random_forest)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)

## Boosting

1. take many weak predictors
2. wight them and add them up
3. get a stronger predictor

#### basic idea

1. start with classifiers
    * all possible trees, all regressions, all cutoffs.

2. Create a classififer that combines classification funcitons
    * goal to minimize error on the training set
    * iterative, select one h at each step
    * calculate weights based on errors
    * upweight missed classifications and select next h
    
[Adaboost on Wikipedia](http://en.wikipedia.org/wiki/AdaBoost)

[http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)


### boosting can be with different models


* Boosting can be used with any subset of classifiers
* One large subclass is [gradient boosting](http://en.wikipedia.org/wiki/Gradient_boosting)
* R has multiple boosting libraries. Differences include the choice of basic classification functions and combination rules.
  * [gbm](http://cran.r-project.org/web/packages/gbm/index.html) - boosting with trees.
  * [mboost](http://cran.r-project.org/web/packages/mboost/index.html) - model based boosting
  * [ada](http://cran.r-project.org/web/packages/ada/index.html) - statistical boosting based on [additive logistic regression](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1016218223)
  * [gamBoost](http://cran.r-project.org/web/packages/GAMBoost/index.html) for boosting generalized additive models
* Most of these are available in the caret package 

```{r wageexample}
require(ISLR)
data(Wage)
require(ggplot2)
require(caret)
Wage <- subset(Wage, select = -c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
```

#### boosting with trees
```{r boostingwithrees}
modFit <- train(wage~., method = "gbm", data = training, verbose = FALSE)

print(modFit)
plot(modFit)
```

```{r}
require(ggplot2)
qplot(predict(modFit,testing), wage, data = testing)

mad(predict(modFit,testing)- testing$wage)
library(MLmetrics)
library(caret)
MLmetrics::RMSE(y_pred = predict(modFit,testing),y_true = testing$wage)

#randomForest compare
modFitrf <- train(wage~., method = "rf", data = training, prox = TRUE)
mad(predict(modFitrf,testing)- testing$wage)
MLmetrics::RMSE(y_pred = predict(modFitrf,testing),y_true = testing$wage)
qplot(predict(modFitrf,testing), wage, data = testing)
```
### Boosting notes and further reading

* A couple of nice tutorials for boosting
  * Freund and Shapire - [http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf](http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf)
  * Ron Meir- [http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)
* Boosting, random forests, and model ensembling are the most common tools that win Kaggle and other prediction contests. 
  * [http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf)
  * [https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf](https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf)
  
## model based predictions
### Basic idea

1. Assume the data follow a probabilistic model
2. Use Bayes' theorem to identify optimal classifiers

__Pros:__

* Can take advantage of structure of the data
* May be computationally convenient
* Are reasonably accurate on real problems

__Cons:__

* Make additional assumptions about the data
* When the model is incorrect you may get reduced accuracy

---

### Model based approach


1. Our goal is to build parametric model for conditional distribution $P(Y = k | X = x)$

2. A typical approach is to apply [Bayes theorem](http://en.wikipedia.org/wiki/Bayes'_theorem):
$$ Pr(Y = k | X=x) = \frac{Pr(X=x|Y=k)Pr(Y=k)}{\sum_{\ell=1}^K Pr(X=x |Y = \ell) Pr(Y=\ell)}$$
$$Pr(Y = k | X=x) = \frac{f_k(x) \pi_k}{\sum_{\ell = 1}^K f_{\ell}(x) \pi_{\ell}}$$
$f_k(x)$ - features distribution given class

$Pr(Y=k)$ -  assume a prior that each specific element comes from a class
3. Typically prior probabilities $\pi_k$ are set in advance.

4. A common choice for $f_k(x) = \frac{1}{\sigma_k \sqrt{2 \pi}}e^{-\frac{(x-\mu_k)^2}{\sigma_k^2}}$, a Gaussian distribution

5. Estimate the parameters ($\mu_k$,$\sigma_k^2$) from the data.

6. Classify to the class with the highest value of $P(Y = k | X = x)$

## Classifying using the model

A range of models use this approach

* Linear discriminant analysis assumes $f_k(x)$ is multivariate Gaussian with same covariances (http://statweb.stanford.edu/~tibs/ElemStatLearn/)
    * basically lines through data
* Quadratic discrimant analysis assumes $f_k(x)$ is multivariate Gaussian with different covariances
    * different covariance matrices through different classes
* [Model based prediction](http://www.stat.washington.edu/mclust/) assumes more complicated versions for the covariance matrix 
* Naive Bayes assumes independence between features for model building

http://statweb.stanford.edu/~tibs/ElemStatLearn/


## Naive Bayes.

Suppose we have many predictors, we would want to model: $P(Y = k | X_1,\ldots,X_m)$

We could use Bayes Theorem to get:

$$P(Y = k | X_1,\ldots,X_m) = \frac{\pi_k P(X_1,\ldots,X_m| Y=k)}{\sum_{\ell = 1}^K P(X_1,\ldots,X_m | Y=k) \pi_{\ell}}$$
$$ \propto \pi_k P(X_1,\ldots,X_m| Y=k)$$

This can be written:

$$P(X_1,\ldots,X_m, Y=k) = \pi_k P(X_1 | Y = k)P(X_2,\ldots,X_m | X_1,Y=k)$$
$$ = \pi_k P(X_1 | Y = k) P(X_2 | X_1, Y=k) P(X_3,\ldots,X_m | X_1,X_2, Y=k)$$
$$ = \pi_k P(X_1 | Y = k) P(X_2 | X_1, Y=k)\ldots P(X_m|X_1\ldots,X_{m-1},Y=k)$$


assume that all of the predictor variables are independent of each other.

In which case they drop out of this conditioning argument.

prior probability X the probability of each feature by itself X conditional on being in each class.


$$ \approx \pi_k P(X_1 | Y = k) P(X_2 | Y = k)\ldots P(X_m |,Y=k)$$

Great when a lot of features - like documents.
```{r modelbased}
data(iris)
library(ggplot2)
names(iris)
inTrain <- createDataPartition(y = iris$Species, p =0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]


modlda=train(Species~., data=training, method = "lda")
modnb=train(Species~., data=training, method = "nb")

plda=predict(modlda, testing)
pnb=predict(modnb, testing)
table(plda, testing$Species)
table(pnb, testing$Species)
```

#### almost identical
```{r bayescomp}
table(pnb, plda)
equalpredictions = (plda== pnb)
qplot(Petal.Width, Sepal.Width, colour=equalpredictions, data = testing)

correctPredition = (pnb == testing$Species)
qplot(Petal.Width, Sepal.Width, colour=correctPredition, data = testing)

```

## Notes and further reading

* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)
* [Elements of Statistical Learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Model based clustering](http://www.stat.washington.edu/raftery/Research/PDF/fraley2002.pdf)
* [Linear Discriminant Analysis](http://en.wikipedia.org/wiki/Linear_discriminant_analysis)
* [Quadratic Discriminant Analysis](http://en.wikipedia.org/wiki/Quadratic_classifier)

# quiz week 3

## 1
```{r}
library(ElemStatLearn)
library(pgmm)
library(rpart)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)

testing <- segmentationOriginal[segmentationOriginal$Case=="Test",]
training <- segmentationOriginal[segmentationOriginal$Case=="Train",]
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)

library(rattle)
library(dplyr)
fancyRpartPlot(modFit$finalModel)
test1 <- testing %>% select(TotalIntenCh2, FiberWidthCh1,PerimStatusCh1, VarIntenCh4) %>% head( n=0) 
test1[1,] <- c(23000, 10,2, 10)
test2 <- testing[1:10,]
arow <- data.frame(TotalIntenCh2=23000, FiberWidthCh1=10, PerimStatusCh1=2)
library(plyr)
test2 <- rbind.fill(test2, arow)
length(predict(modFit, test2))
dim(test2)
```

a PS. TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2

b WS. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100

c PS. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100

d not possible. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2

## 2
small k - large bias, small variance.
leave one out k = samples size

## 3
not strange
```{r}
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))

olivearea <- train(Area ~ .,method="rpart",data=olive)
fancyRpartPlot(olivearea$finalModel)
predict(olivearea, newdata)
```

## 4
```{r}
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)

logfitter <- train(factor(chd)~age+alcohol+obesity+tobacco+typea+ldl, method="glm", family = binomial, data =trainSA)

missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}

predtst <- predict(logfitter,testSA)
predtrain <- predict(logfitter, trainSA)
missClass(testSA$chd,as.numeric(levels(predtst))[predtst])
missClass(trainSA$chd,as.numeric(levels(predtrain))[predtrain])
```

## 5
```{r}
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test <- vowel.test %>% mutate(y = factor(y))
vowel.train <- vowel.train %>% mutate(y = factor(y))
library(randomForest)

set.seed(33833)
forestrun <- randomForest::randomForest(y~., data= vowel.train)
plot(forestrun)
varImp(forestrun) %>% View()
```
21568493

