---
title: "Practical machine learning  lectures 4"
author: "Kirill Setdekov"
date: "December 20 2019"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	cache = TRUE
)
```

# week 4

## reguralized regressions

_idea_

1. fit regression
2. shrink large coefficients.

*good*

* can help with bias
* can help with model selection

*bad*

* may be computationally hard on big data
* worse than random forest and boosting

```{r}
library(ElemStatLearn)
data("prostate")
str(prostate)
```

<img class="center" src="prostate.png" height="450">

[Code here](http://www.cbcb.umd.edu/~hcorrada/PracticalML/src/selection.R)

## Most common pattern

<img class="center" src="trainingandtest.png" height="450">

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/


## good solution - split samples
* No method better when data/computation time permits it

* Approach
  1. Divide data into training/test/validation
  2. Treat validation as test data, train all competing models on the train data and pick the best one on validation. 
  3. To appropriately assess performance on new data apply to test set
  4. You may re-split and reperform steps 1-3

* Two common problems
  * Limited data
  * Computational complexity
  
http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/

Reqularized regression trades of variance for better bias

Assume $Y_i = f(X_i) + \epsilon_i$

$EPE(\lambda) = E\left[\{Y - \hat{f}_{\lambda}(X)\}^2\right]$

Suppose $\hat{f}_{\lambda}$ is the estimate from the training data and look at a new data point $X = x^*$

$$E\left[\{Y - \hat{f}_{\lambda}(x^*)\}^2\right] = \sigma^2 + \{E[\hat{f}_{\lambda}(x^*)] - f(x^*)\}^2 + var[\hat{f}_\lambda(x_0)]$$

<center> = Irreducible error + Bias$^2$ + Variance </center>

#### One more issue with high-dimensional data

More predictors than samples

```{r small}
small = prostate[1:5, ]
lm(lpsa~., data = small)
```
## Hard thresholding

* Model $Y = f(X) + \epsilon$

* Set $\hat{f}_{\lambda}(x) = x'\beta$

* Constrain only $\lambda$ coefficients to be nonzero. 

* Selection problem is after chosing $\lambda$ figure out which $p - \lambda$ coefficients to make nonzero

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/

---

## Regularization for regression

If the $\beta_j$'s are unconstrained:
* They can explode
* And hence are susceptible to very high variance

To control variance, we might regularize/shrink the coefficients. 

$$ PRSS(\beta) = \sum_{j=1}^n (Y_j - \sum_{i=1}^m \beta_{1i} X_{ij})^2 + P(\lambda; \beta)$$

where $PRSS$ is a penalized form of the sum of squares. Things that are commonly looked for

* Penalty reduces complexity
* Penalty reduces variance
* Penalty respects structure of the problem

---

## Ridge regression

Solve:

$$ \sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p \beta_j^2$$

equivalent to solving

$\sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2$ subject to $\sum_{j=1}^p \beta_j^2 \leq s$ where $s$ is inversely proportional to $\lambda$ 


Inclusion of $\lambda$ makes the problem non-singular even if $X^TX$ is not invertible.

http://www.biostat.jhsph.edu/~ririzarr/Teaching/649/
http://www.cbcb.umd.edu/~hcorrada/PracticalML/

## Tuning parameter $\lambda$

* $\lambda$ controls the size of the coefficients
* $\lambda$ controls the amount of {\bf regularization}
* As $\lambda \rightarrow 0$ we obtain the least square solution
* As $\lambda \rightarrow \infty$ we have $\hat{\beta}_{\lambda=\infty}^{ridge} = 0$


## Lasso 

$\sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2$ subject to $\sum_{j=1}^p |\beta_j| \leq s$ 

also has a lagrangian form 

$$ \sum_{i=1}^N \left(y_i - \beta_0 + \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda \sum_{j=1}^p |\beta_j|$$

For orthonormal design matrices (not the norm!) this has a closed form solution

$$\hat{\beta}_j = sign(\hat{\beta}_j^0)(|\hat{\beta}_j^0 - \gamma)^{+}$$
 
but not in general. 

Good model as it 

## Notes and further reading


* [Hector Corrada Bravo's Practical Machine Learning lecture notes](http://www.cbcb.umd.edu/~hcorrada/PracticalML/)
* [Hector's penalized regression reading list](http://www.cbcb.umd.edu/~hcorrada/AMSC689.html#readings)
* [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)
* In `caret` methods are:
  * `ridge`
  * `lasso`
  * `relaxo`
  
```{r warning=FALSE}

library(dplyr)

library(caret)
require(ISLR)
data(Wage)
require(ggplot2)

Wage <- subset(Wage, select = -c(logwage))
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
modFit <- train(wage~., method = "lm", data = training, verbose = FALSE)

print(modFit)
```

Это результат для простой модели

```{r warning=FALSE}
qplot(predict(modFit,testing), wage, data = testing)
MLmetrics::RMSE(y_pred = predict(modFit,testing),y_true = testing$wage)

lasso_caret<- train(wage~., data = training, method = "glmnet", lambda= 0,
                    tuneGrid = expand.grid(alpha = 1,  lambda = 0))
print(lasso_caret)
qplot(predict(lasso_caret,testing), wage, data = testing)
MLmetrics::RMSE(y_pred = predict(lasso_caret,testing),y_true = testing$wage)


```
Это результат для модели с Lasso



# ensembling methods

* combine classifiers by averaging, Voting
* combininig -> greater accuracy
* reduce interpretability
* boosting, bagging and RF does this with one type of predictions

## approaches

1. boosting, bagging and RF 
    * similar classifier
2. combine different classifiers
    * model stacking 
    * model ensembling

```{r wageex}
#separate in 3 sets
library(ISLR)
data("Wage")
library(ggplot2)
library(caret)
Wage <- subset(Wage, select = -c(logwage))
inBuild <- createDataPartition(y=Wage$wage, p=0.7, list = FALSE)
validation <- Wage[-inBuild,]
buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage, p=0.7, list = FALSE)
training <- buildData[inTrain,]
testing <- buildData[-inTrain,]
```

### build 2 models

```{r warning=FALSE}
library(party)

mod1 <- train(wage~., method ="glm", data = training)
mod2 <- train(wage~., method ="rf", data = training, trControl = trainControl(method = "cv"),number = 3)
mod3 <- train(wage~., method ="rpart", data = training)
mod4 <- train(wage~., method ="ctree", data = training)
mod5 <- mob(wage~age+education|year+maritl+race+region+jobclass+health+health_ins,data = training)

```
### compare them
```{r dependson=c(-1)}
pred1 <- predict(mod1, testing)
pred2 <- predict(mod2, testing)
pred3 <- predict(mod3, testing)
pred4 <- predict(mod4, testing)
pred5 <- predict(mod5, testing)
                 
qplot(pred1,pred2, colour = wage, data = testing)
qplot(pred3,pred4, colour = wage, data = testing)


library(rattle)
fancyRpartPlot(mod3$finalModel)

plot(mod4$finalModel)
plot(mod5)

qplot(wage,pred5, colour = wage, data = testing)

```

### make a model that combines predictors

```{r dependson=c(-1, -2)}
predDF2 <- data.frame(pred1, pred2, wage = testing$wage)
#variant with 5
predDF5 <- data.frame(pred1, pred2, pred3, pred4, pred5, wage = testing$wage)

combModFit2 <- train(wage~., method = "gam", data = predDF2)
combModFit5 <- train(wage~., method = "gam", data = predDF5)
combPred2 <- predict(combModFit2,predDF2)
combPred5 <- predict(combModFit5,predDF2)

```

#### testing errors
```{r}
MLmetrics::RMSE(y_pred = pred1,y_true = testing$wage)
MLmetrics::RMSE(y_pred = pred2,y_true = testing$wage)
MLmetrics::RMSE(y_pred = combPred2,y_true = testing$wage)

MLmetrics::RMSE(y_pred = pred3,y_true = testing$wage)
MLmetrics::RMSE(y_pred = pred4,y_true = testing$wage)
MLmetrics::RMSE(y_pred = pred5,y_true = testing$wage)
MLmetrics::RMSE(y_pred = combPred5,y_true = testing$wage)

plot(combModFit5$finalModel)
```
### on a validation
```{r}
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
predVDF2 <- data.frame(pred1 = pred1V, pred2 = pred2V)
combPredV2 <- predict(combModFit2, predVDF2)

pred3V <- predict(mod3, validation)
pred4V <- predict(mod4, validation)
pred5V <- predict(mod5, validation)
predVDF5 <-
    data.frame(
        pred1 = pred1V,
        pred2 = pred2V,
        pred3 = pred3V,
        pred4 = pred4V,
        pred5 = pred5V
    )
combPredV5 <- predict(combModFit5,predVDF5)
MLmetrics::RMSE(y_pred = pred1V,y_true = validation$wage)
MLmetrics::RMSE(y_pred = pred2V,y_true = validation$wage)
MLmetrics::RMSE(y_pred = combPredV2,y_true = validation$wage)
## 5 part model
MLmetrics::RMSE(y_pred = pred3V,y_true = validation$wage)
MLmetrics::RMSE(y_pred = pred4V,y_true = validation$wage)
MLmetrics::RMSE(y_pred = pred5V,y_true = validation$wage)
MLmetrics::RMSE(y_pred = combPredV5,y_true = validation$wage)
```

## Notes and further resources

* Even simple blending can be useful
* Typical model for binary/multiclass data
  * Build an odd number of models
  * Predict with each model
  * Predict the class by majority vote
* This can get dramatically more complicated
  * Simple blending in caret: [caretEnsemble](https://github.com/zachmayer/caretEnsemble) (use at your own risk!)
  * Wikipedia [ensemlbe learning](http://en.wikipedia.org/wiki/Ensemble_learning)

---

## Recall - scalability matters

[http://www.techdirt.com/blog/innovation/articles/20120409/03412518422/](http://www.techdirt.com/blog/innovation/articles/20120409/03412518422/)

[http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)

# Forecasting

* Data dependent over time
* Harder to subsempling for training/testing
* mind what you predict

## quantmod and google data
```{r}
library(quantmod)
from.dat <- as.Date("01/01/08", format = "%m/%d/%y")
to.dat <- as.Date("12/24/19", format = "%m/%d/%y")
getSymbols("RUB=X", src = "yahoo", from = from.dat, to = to.dat)
tail(`RUB=X`)
```
## summarize
```{r}
rub <- to.daily(`RUB=X`)
rubM <- to.monthly(`RUB=X`)
rubclose <- Op(rub)
rubclosem <- Op(rubM)
ts1 <- ts(rubclose, frequency = 259)
tsm <- ts(rubclosem, frequency = 12)

plot(ts1, xlab = "Days+1", ylab = "USD/RUB")
plot(tsm, xlab = "Years+1", ylab = "USD/RUB")

to.dat-from.dat
3097*365/4375
```
## decompose
```{r}
plot(decompose(tsm), xlab = "Years+1")
plot(decompose(ts1), xlab = "Years+1")
```

## train and test sets
```{r}
tsmTrain <- window(tsm, start =1, end = 11)
tsmTest <- window(tsm, start = 11, end = (12-0.01))
tsmTrain
```
## simple fx MA
```{r}
plot(tsmTrain)
library(forecast)
lines(ma(tsmTrain, order = 4), col = "red")
```


__Example - simple exponential smoothing__
$$\hat{y}_{t+1} = \alpha y_t + (1-\alpha)\hat{y}_{t-1}$$

[https://www.otexts.org/fpp/7/6](https://www.otexts.org/fpp/7/6)

## exponential smoothing
```{r}
etsm <- ets(tsmTrain, model = "MMM")
fcast <- forecast(etsm)
plot(fcast)
lines(tsmTest, col = "red")


fit <- hw(tsmTrain)
plot(forecast(fit))
lines(tsmTest, col = "red")
```

```{r}
accuracy(fcast, tsmTest)
```
## Notes and further resources

* [Forecasting and timeseries prediction](http://en.wikipedia.org/wiki/Forecasting) is an entire field
* Rob Hyndman's [Forecasting: principles and practice](https://www.otexts.org/fpp/) is a good place to start
* Cautions
  * Be wary of spurious correlations
  * Be careful how far you predict (extrapolation)
  * Be wary of dependencies over time
* See [quantmod](http://cran.r-project.org/web/packages/quantmod/quantmod.pdf) or [quandl](http://www.quandl.com/help/packages/r) packages for finance-related problems.


# Unsupervised predictions
## Key ideas

* Sometimes you don't know the labels for prediction
* To build a predictor
  * Create clusters - somehow
  * Name clusters - hard (not known)
  * Build predictor for clusters
* In a new data set
  * Predict clusters

```{r iris}
data(iris); library(ggplot2)
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
```


---

## Cluster with k-means

```{r kmeans,dependson="iris",fig.height=4,fig.width=6}
kMeans1 <- kmeans(subset(training,select=-c(Species)),centers=5)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width,Petal.Length,colour=clusters,data=training)
```

## Compare to real labels

```{r ,dependson="kmeans"}
table(kMeans1$cluster,training$Species)
```
## Build predictor

```{r modelFit,dependson="kmeans"}
modFit <- train(clusters ~.,data=subset(training,select=-c(Species)),method="ctree")
table(predict(modFit,training),training$Species)
```

---

## Apply on test

```{r ,dependson="modFit"}
testClusterPred <- predict(modFit,testing) 
table(testClusterPred ,testing$Species)
```


---

## Notes and further reading

* _The cl_predict function in the clue package_ provides similar functionality
* Beware over-interpretation of clusters!
* This is one basic approach to [recommendation engines](http://en.wikipedia.org/wiki/Recommender_system)
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)
* [Introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/)

# Quiz
### 1 (0.6 0.51 0.636)
```{r}
library(ElemStatLearn)
library(caret)
data(vowel.train)

data(vowel.test)
library(dplyr)
vowel.train <- vowel.train %>% mutate(y = factor(y))
vowel.test <- vowel.test %>% mutate(y = factor(y))

set.seed(33833)

mod1 <- train(y~., method ="rf", data = vowel.train, verbose = FALSE)
mod2 <- train(y~., method ="gbm", data = vowel.train, verbose = FALSE)


prediction1 <- predict(mod1, newdata = vowel.test)
confusionMatrix(prediction1, vowel.test$y)
#rf 0.5974
prediction2 <- predict(mod2, newdata = vowel.test)
confusionMatrix(prediction2, vowel.test$y)
#rf 0.5368
confusionMatrix(prediction1, prediction2)
# agreement 0.697
```
### 2 0.76 worse than lda

```{r}
library(caret)

library(gbm)

set.seed(3433)

library(AppliedPredictiveModeling)

data(AlzheimerDisease)

adData = data.frame(diagnosis,predictors)

inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]

training = adData[ inTrain,]

testing = adData[-inTrain,]

set.seed(62433)
mod1 <- train(diagnosis~., method ="rf", data = training, verbose = FALSE)
mod2 <- train(diagnosis~., method ="gbm", data = training, verbose = FALSE)
mod3 <- train(diagnosis~., method ="lda", data = training, verbose = FALSE)

pred1 <- predict(mod1, training)
pred2 <- predict(mod2, training)
pred3 <- predict(mod3, training)
predDF3 <- data.frame(pred1, pred2, pred3, diagnosis = training$diagnosis)



combModFit <- train(diagnosis~., method = "rf", data = predDF3, verbose = FALSE)

pred1 <- predict(mod1, testing)
pred2 <- predict(mod2, testing)
pred3 <- predict(mod3, testing)
predDF3 <- data.frame(pred1, pred2, pred3, diagnosis = testing$diagnosis)
combPred <- predict(combModFit,predDF3)

MLmetrics::Accuracy(combPred, testing$diagnosis)


MLmetrics::Accuracy(pred1, testing$diagnosis)
MLmetrics::Accuracy(pred2, testing$diagnosis)
MLmetrics::Accuracy(pred3, testing$diagnosis)

```

### 3 cement

```{r}
set.seed(3523)

library(AppliedPredictiveModeling)

data(concrete)

inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]

training = concrete[ inTrain,]

testing = concrete[-inTrain,]

set.seed(233)


library(elasticnet)
object <- enet(x = as.matrix(training[,1:8]),training[,9],lambda=0)

plot(object,xvar="step", use.color = TRUE)

```

### 4 0.9617  
```{r}
library(lubridate) # For year() function below

dat = read.csv("gaData.csv")

training = dat[year(dat$date) < 2012,]

testing = dat[(year(dat$date)) > 2011,]

tstrain = ts(training$visitsTumblr)
library(forecast)

tspredict <- bats(training$visitsTumblr)
testfx <- forecast(tspredict,h = 235)
testfx %>% plot()

upper <- testfx$upper[,2]
lower <- testfx$lower[,2]

testpref <- data.frame(upper, lower, visitors=testing$visitsTumblr)
testpref %>% mutate(within = ifelse((visitors>lower)&(visitors<upper),1,0)) %>% select(within) %>% summary()
```


### 5 7.962075 (6.93)
Set the seed to 325 and fit a support vector machine using the e1071 package to predict Compressive Strength using the default settings. Predict on the testing set. What is the RMSE?
```{r}
set.seed(3523)

library(AppliedPredictiveModeling)

data(concrete)

inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]

training = concrete[ inTrain,]

testing = concrete[-inTrain,]
set.seed(325)
library(e1071)

svmresult <- e1071::svm(CompressiveStrength~., data = training)

MLmetrics::RMSE(y_pred = predict(svmresult,testing),y_true = testing$CompressiveStrength)
```


