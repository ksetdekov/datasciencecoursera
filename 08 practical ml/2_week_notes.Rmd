---
title: "Practical machine learning  lectures 2"
author: "Kirill Setdekov"
date: "October 05 2019"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	cache = TRUE
)
```

# week 2

Caret package

## additional materisl


* Caret tutorials:
    * [http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
    * [https://cran.r-project.org/web/packages/caret/vignettes/caret.html](https://cran.r-project.org/web/packages/caret/vignettes/caret.html)
* A paper introducing the caret package
    * [http://www.jstatsoft.org/v28/i05/paper](http://www.jstatsoft.org/v28/i05/paper)

```{r caret}
require(caret)
```

Functionality

* cleaning
    * preProcess
* data splitting
    * createDataPartition
    * createResample
    * createTimeSlices
* trainign/testing functionality
    * train
    * predict
* model comparison
    * confusionMatrix
```{r spamsplit}
require(kernlab)
data(spam)
intrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[intrain, ]
testing <- spam[-intrain, ]
dim(training)

#fit a model
set.seed(32343)
modelFit <- train(type ~ ., data = training, method = "glm")
modelFit

#results
modelFit$finalModel

#predict
prediction <- predict(modelFit, newdata = testing)
head(prediction)
table(prediction)

#confusionmatrix
confusionMatrix(prediction, testing$type)
```

```{r dataslicing_kfold}
require(kernlab)
data(spam)
intrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[intrain, ]
testing <- spam[-intrain, ]
dim(training)

fold <- createFolds(y=spam$type, k=10, list = TRUE, returnTrain = TRUE)
sapply(fold, length)

#return only testing
fold <- createFolds(y=spam$type, k=10, list = TRUE, returnTrain = FALSE)
sapply(fold, length)

#doing resampling the whole set
fold <- createResample(y=spam$type, times=10, list = TRUE)
sapply(fold, length)


# time clices
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow = 20, horizon = 10)
names(folds)
folds$train[[1]]
```

### Training options
Spam example
```{r trainingcontrolspam}
require(caret)
require(kernlab)
data(spam)

intrain <- createDataPartition(y=spam$type, p=0.75, list = FALSE)
training <- spam[intrain,]
testing <- spam[-intrain,]
modelFit <- train(type~., data = training, method ="glm")

#results
modelFit$finalModel

#predict
prediction <- predict(modelFit, newdata = testing)
head(prediction)
table(prediction)

#confusionmatrix
confusionMatrix(prediction, testing$type)

# accuracy to Kappa
modelFit <- train(type~., data = training, method ="glm", metric = "Kappa")
prediction2 <- predict(modelFit, newdata = testing)
confusionMatrix(prediction2, testing$type)



```

Metric options
* Continuous
    * RMSE
    * R^2
* categorical
    * accuracy
    * Kappa
    
```{r}
args(trainControl)
control <- trainControl(number = 100)
modelFit <- train(type~., data = training, method ="glm", trControl=control)
prediction <- predict(modelFit, newdata = testing)
confusionMatrix(prediction, testing$type)
```
Method

* boot bootstrap
* boot632 bootstrap  with adjustment
* cv cross validation
* repeatedcv = repeated cross validation
* loocv = leave one out cross validation

number - usefull for many parameters
 
* for boot/cross validaiton
* number of subsaples to take

repears

* number of times to repaea subsampling
* if big this can slow things down a lot

```{r}
control <- trainControl(method = "cv")
modelFit <- train(type~., data = training, method ="glm", trControl=control)
prediction <- predict(modelFit, newdata = testing)
confusionMatrix(prediction, testing$type)
```
Usefull to set a seed.

```{r setseedresult}
set.seed(1235)
modelFit3 <- train(type~., data=training, method="glm")
modelFit3
```

# plotting predicitons

Wage data
```{r wages}
library(ISLR)
require(ggplot2)
require(caret)
require(GGally)
require(dplyr)
data(Wage)
summary(Wage)

pairs(Wage)
Wage %>% select(age, race, wage, education) %>% ggpairs(., lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1)))

#set adide testing set
intrain <- createDataPartition(y=Wage$wage, p=0.7, list = FALSE)
training <- Wage[intrain,]
testin <- Wage[-intrain,]
dim(training)
dim(testin)
```

## Feature plot ::caret
```{r plots}
featurePlot(x = training[,c("age","race", "education")], y = training$wage, plot = "pairs")

qplot(age, wage, colour = education,data = training)+geom_smooth(method = 'lm', formula = y~x)
```

```{r cut}
require(Hmisc)
require(gridExtra)
cutwage <- cut2(training$wage, g=3)
table(cutwage)

cbind.data.frame(cutwage,training %>% select(race)) %>% ggpairs(., lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1)))
cbind.data.frame(cutwage,training %>% select(race)) %>% table() %>% prop.table(.,2)

qplot(cutwage, age, data = training, fill = cutwage, geom = c("violin"))

qplot(cutwage, age, data = training, fill = cutwage, geom = c("boxplot"))

p1 <- qplot(race, wage, data = training, fill = education, geom = c("boxplot"))


p1
p2 <-
qplot(
race,
wage,
data = training,
fill = education, color = education,
geom = c("boxplot", "jitter")
)
grid.arrange(p1, p2, ncol = 2)
```

### tables

```{r tables}
table(cutwage,training$jobclass) %>% prop.table(.,1)
```

### density plot

```{r density}
qplot(wage, colour= education, data = training, geom = "density")

qplot(wage, colour= race, data = training, geom = "density")
```

#### Notes 

* Make plots on the training set
    * dont explore testing
* look for
    * imbalances in outcome/predictors
    * otliers
    * groups of points not explained by a predictor
    * skewed variables

# preprocessing

needed for model -bases approaches.
## why preprocess?

```{r preprocess}
require(caret)
require(kernlab)
data("spam")
intrain <- createDataPartition(y=spam$type, p=0.75, list = FALSE)
training <- spam[intrain,]
testing <- spam[-intrain,]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")

qplot(type, log10(training$capitalAve), data = training, geom = "violin")

mean(training$capitalAve)
sd(training$capitalAve)
```

## standardize
```{r}
traincapave <- training$capitalAve
traincapaves <- (traincapave-mean(traincapave))/sd(traincapave)

summary(traincapaves)
sd(traincapaves)
```

standardisation will be done by the training set.

```{r}
testcapave <- testing$capitalAve
testcapaves <- (testcapave-mean(traincapave))/sd(traincapave)

summary(testcapaves)
sd(testcapaves)
```

### caret has preporcessing
this is standardization - without writing formulas

```{r caretpreporcess}
preObj <- preProcess(training[,-58], method = c("center","scale"))
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
```

can use this to apply to the testing
```{r}
testcapaves <- predict(preObj,testing[,-58])$capitalAve
mean(testcapaves)
sd(testcapaves)
```

can pass preProcess commands to train
```{r}
modelFit <- train(type~., data = training, preProcess = c("center","scale"), method = "glm")
modelFit
```

### Other transformations
```{r boxcox}
preObj <- preProcess(training[-58], method = c("BoxCox"))
trainCapAveS <- predict(preObj,training[-58])$capitalAve
par(mfrow = c(1,2))
hist(trainCapAveS)
qqnorm(trainCapAveS)

par(mfrow = c(1,2))
hist(training$capitalAve)
qqnorm(training$capitalAve)
```

### imputing data - helping with missing data

find neerest k rows with missing data - and averages them 


```{r imputingdata}
set.seed(42)

# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA

# Impute and standardize
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve

# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)


quantile(capAve- capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
```

* Trainign and testing must be processed in the same way
* test trainsformation will likely be imperfect
    * Especially if data sets collected at different times
* Be carefull with factor variables
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)

# covariate creating

1. from raw data to covariate (email to letter counts)
    * depends heavy on application
    * balancing act is summarization vs information loss
    * examples
        * text files - frequence of words, frequence of phrased, google ngrams, frequency of capitals
        * images - edges, cornergs, blobs, ridges
        * webpages - numbers and types of images, position of elements, colors, videos - A/B testing
        * people - height, wight, hair color, sex, country of origin
    * need more knowledge
    * better err on the side of more features
    * can be automated but use caution
2. transforming tidy covariates
    * features already created
    * more necessary for metthonds (regression, svms) that others (classification trees)
    * should be done only _on the training set_
    * best approach is through exploratory analysis (plotting/tables)
    * new covariates should be added to data frames
```{r}
require(kernlab)
data("spam")
spam$capitalAvesq <- spam$capitalAve^2
par(mfrow = c(1,2))
hist(spam$capitalAvesq)
qqnorm(spam$capitalAvesq)
```

```{r}
library(ISLR)
library(caret)
data("Wage")
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list = F)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
```
Commong covarites to add, dummy variables
convert factor variables to indicator variables

```{r}
table(training$jobclass)

dummies <- dummyVars(wage~jobclass, data = training)
head(predict(dummies, newdata=training))
```

Removing zero covariates
```{r}
nsv <- nearZeroVar(training, saveMetrics = T)
nsv
```

spline basis
```{r}
require(splines)
bsBasis <- bs(training$age, df=3)
head(bsBasis)
```
helps with curvy model fitting

## fitting curves with splines
fits 3 order polynomial
```{r}
lm1 <- lm(wage~bsBasis, data = training)
plot(training$age, training$wage, pch=19, cex=0.5)
points(training$age, predict(lm1, newdata=training), col="red", pch=19, cex=0.5)
```

## splines on the test set
```{r}
head(predict(bsBasis, age = testing$age))
```

## Notes and further reading

* Level 1 feature creation (raw data to covariates)
  * Science is key. Google "feature extraction for [data type]"
  * Err on overcreation of features
  * In some applications (images, voices) automated feature creation is possible/necessary
    * http://www.cs.nyu.edu/~yann/talks/lecun-ranzato-icml2013.pdf
* Level 2 feature creation (covariates to new covariates)
  * The function _preProcess_ in _caret_ will handle some preprocessing.
  * Create new covariates if you think they will improve fit
  * Use exploratory analysis on the training set for creating them
  * Be careful about overfitting!
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)
* If you want to fit spline models, use the _gam_ method in the _caret_ package which allows smoothing of multiple variables.
* More on feature creation/data tidying in the Obtaining Data course from the Data Science course track. 

# principal componen analysys
```{r}
require(caret)
require(kernlab)
data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M>0.8, arr.ind = T)

names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
```

Both can be not useful
* wee might not need every
* a weighted combination of predictors might be better
* pick combinations for "most information" possible
* benefits
  * reduce predictor number
  * reduce noise

we could rotate the plot
```{r rotate}
X <- 0.71 * training$num415 + 0.71 * training$num857
Y <- 0.71 * training$num415 - 0.71 * training$num857
plot(X,Y)
```

Related problems.

* find a new set of multivariate variables that are uncorrelated and explain as much variance as possible - statistical goal
* if you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explans the original data. - data compression goal

* Related solutions - PCA/SVD
_SVD_
Variables in V - are constructed to explain the maximum amount of variation in the data.

If X is a matrix with each variable in  clumn and each observation in a row then SVD is a "matrix decomposition".

$$ X=UDV^T $$
whre the columns of $U$ are orthogonal (left singular vectors), the columns of V are orthogonal (right singular vectors) and D is a diagonal matrix (singular values)

_PCA_
THe pricnipal components are equal to the right singular values if you first standardize the variables.

```{r prcompex}
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
```
PCA on 2 varables - look like adding and substracting X and Y combinations from above.

```{r}
prComp$rotation
require(dplyr)
prcompTr <- training[,-58] %>% prcomp()
prcompTr$rotation
```
PCA on SPAM
```{r}
typeColor <- ((training$type=="spam")*1+1)
prComp <- prcomp(log10(training[,-58]+1))
plot(prComp$x[,1],prComp$x[,2], col=typeColor, xlab="PC1", ylab = "PC2")
```

## PCA with caret
```{r}
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
```

```{r}
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(x = trainPC, y = training$type,method="glm")

testPC <- predict(preProc, log10(testing[,-58]+1))
confusionMatrix(testing$type, predict(modelFit, testPC))
```
## alternative (sets # of PCs)
```{r}

modelFit <- train(type ~.,method="glm", preProcess="pca", data = training)
confusionMatrix(testing$type,predict(modelFit,testing))
```

* Most useful for linear-type models
* Can make it harder to interpret predictors
* Watch out for outliers! 
  * Transform first (with logs/Box Cox)
  * Plot predictors to identify problems
* For more info see 
  * Exploratory Data Analysis
  * [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)
  
# predicting with regressions
eruptions on geisers
```{r geisers}
require(caret)
data("faithful")
set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting, p=0.7, list = FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting, trainFaith$eruptions)
```

Fit a linear model.
$ ED_i = b_o+ b_1WT_i+e_i$
```{r}
lm1 <- lm(eruptions~waiting, data = trainFaith)
summary(lm1)
```

Modelfit
```{r}
with(trainFaith, plot(waiting,eruptions))
lines(trainFaith$waiting, lm1$fitted.values, col="blue")
```

Predict new value
```{r}
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
```
training and testing plots
```{r}
par(mfrow=c(1,2))
with(trainFaith, plot(waiting,eruptions))
lines(trainFaith$waiting, lm1$fitted.values, col="blue")
with(testFaith,plot(waiting,eruptions))
lines(testFaith$waiting, predict(lm1, newdata = testFaith), col="blue")
```

get errors RMSE

```{r}
#train 
sqrt(sum((lm1$fitted.values- trainFaith$eruptions)^2))
#test
sqrt(sum((predict(lm1, newdata = testFaith)- testFaith$eruptions)^2))
```

Prediction intervals
```{r}
pred1 <- predict(lm1, newdata = testFaith, interval = "prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting, testFaith$eruptions, col="blue")
matlines(testFaith$waiting[ord], pred1[ord,], type = "l",col = c(1,2,2), lty=c(1,1,1), lwd=3)
```

Same with caret
```{r}
modFit <- train(eruptions~waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
```

Predicting with multiple regressions
```{r}
library(ISLR)
library(ggplot2)
library(caret)
data("Wage")
Wage <- subset(Wage, select = -c(logwage))
summary(Wage)

inTrain <- createDataPartition(y=Wage$wage, p=0.7, list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training)
dim(testing)
featurePlot(x=training[,c("age", "education", "jobclass")], y=training$wage, plot = "pairs")

qplot(age,wage,colour = jobclass,data = training)

qplot(age,wage,colour = education,data = training)
```
FIt a linear model
$$ ED_i = b_0 + b_1 age + b_2 I(Jobclass_i="Information") + \sum_{k=1}^4 \gamma_k I(education_i= level k) $$
```{r}
modFit <- train(wage~age+jobclass+education, method="lm", data=training)
finMod <- modFit$finalModel
print(modFit)
summary(finMod)
plot(finMod, 1, pch=19, cex=0.5)
#label by factor
qplot(finMod$fitted.values, finMod$residuals,colour = race, data=training)
#plot by index
plot(finMod$residuals, pch=19)
```
Predicted vs truth in test set post - analysis
```{r}
pred <- predict(modFit, testing)
qplot(wage, pred, colour=year, data=testing)
```
ideally - need horizontal
```{r}
modFitall <- train(wage~., data=training, method="lm")
pred <- predict(modFitall, testing)
qplot(wage, pred, data=testing)
```

### quiz

```{r}
#1
library(AppliedPredictiveModeling)
data(AlzheimerDisease)

adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]

#2
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
flyash <- cut2(x = testing$FlyAsh, g = 3)
qplot(x = 1:dim(testing)[1],testing$CompressiveStrength, colour = flyash, data=testing)

agecut <- cut2(x = testing$Age, g = 3)
qplot(x = 1:dim(testing)[1],testing$CompressiveStrength, colour = agecut, data=testing)


#3
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(training$Superplasticizer)
summary(training$Superplasticizer)

qplot(log10(training$Superplasticizer+1))

#5

library(caret)
library(AppliedPredictiveModeling)
library(dplyr)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis,p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testingsubset <- training %>% dplyr:: select(starts_with("IL"))


preProc <- preProcess(testingsubset,method="pca",pcaComp=12)
PC <- predict(preProc,testingsubset)
#7

#5
library(caret)
library(AppliedPredictiveModeling)
RNGversion("3.0.0")
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

#pca 0.71 non 0.72


il <- grep("^IL", names(training))

m1 <- train(diagnosis ~ ., data=training[,c(1,il)], method="glm")

ctrl <- trainControl(preProcOptions = list(thresh = 0.8))
m2 <- train(diagnosis ~ ., data=training[,c(1,il)], preProcess = "pca", method="glm", trControl=ctrl)

confusionMatrix(testing$diagnosis, predict(m1, testing))
confusionMatrix(testing$diagnosis, predict(m2, testing))

m2$preProcess
defaultm3 <-  train(diagnosis ~ ., data=training[,c(1,il)], preProcess = "pca", method="glm")
defaultm3$preProcess

```


