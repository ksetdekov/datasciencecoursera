---
title: "Practical machine learning  lectures 2"
author: "Kirill Setdekov"
date: "October 05 2019"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	cache = TRUE
)
```

# week 2

Caret package

## additional materisl


* Caret tutorials:
    * [http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
    * [https://cran.r-project.org/web/packages/caret/vignettes/caret.html](https://cran.r-project.org/web/packages/caret/vignettes/caret.html)
* A paper introducing the caret package
    * [http://www.jstatsoft.org/v28/i05/paper](http://www.jstatsoft.org/v28/i05/paper)

```{r caret}
require(caret)
```

Functionality

* cleaning
    * preProcess
* data splitting
    * createDataPartition
    * createResample
    * createTimeSlices
* trainign/testing functionality
    * train
    * predict
* model comparison
    * confusionMatrix
```{r spamsplit}
require(kernlab)
data(spam)
intrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[intrain, ]
testing <- spam[-intrain, ]
dim(training)

#fit a model
set.seed(32343)
modelFit <- train(type ~ ., data = training, method = "glm")
modelFit

#results
modelFit$finalModel

#predict
prediction <- predict(modelFit, newdata = testing)
head(prediction)
table(prediction)

#confusionmatrix
confusionMatrix(prediction, testing$type)
```

```{r dataslicing_kfold}
require(kernlab)
data(spam)
intrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[intrain, ]
testing <- spam[-intrain, ]
dim(training)

fold <- createFolds(y=spam$type, k=10, list = TRUE, returnTrain = TRUE)
sapply(fold, length)

#return only testing
fold <- createFolds(y=spam$type, k=10, list = TRUE, returnTrain = FALSE)
sapply(fold, length)

#doing resampling the whole set
fold <- createResample(y=spam$type, times=10, list = TRUE)
sapply(fold, length)


# time clices
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow = 20, horizon = 10)
names(folds)
folds$train[[1]]
```

### Training options
Spam example
```{r trainingcontrolspam}
require(caret)
require(kernlab)
data(spam)

intrain <- createDataPartition(y=spam$type, p=0.75, list = FALSE)
training <- spam[intrain,]
testing <- spam[-intrain,]
modelFit <- train(type~., data = training, method ="glm")

#results
modelFit$finalModel

#predict
prediction <- predict(modelFit, newdata = testing)
head(prediction)
table(prediction)

#confusionmatrix
confusionMatrix(prediction, testing$type)

# accuracy to Kappa
modelFit <- train(type~., data = training, method ="glm", metric = "Kappa")
prediction2 <- predict(modelFit, newdata = testing)
confusionMatrix(prediction2, testing$type)



```

Metric options
* Continuous
    * RMSE
    * R^2
* categorical
    * accuracy
    * Kappa
    
```{r}
args(trainControl)
control <- trainControl(number = 100)
modelFit <- train(type~., data = training, method ="glm", trControl=control)
prediction <- predict(modelFit, newdata = testing)
confusionMatrix(prediction, testing$type)
```
Method

* boot bootstrap
* boot632 bootstrap  with adjustment
* cv cross validation
* repeatedcv = repeated cross validation
* loocv = leave one out cross validation

number - usefull for many parameters
 
* for boot/cross validaiton
* number of subsaples to take

repears

* number of times to repaea subsampling
* if big this can slow things down a lot

```{r}
control <- trainControl(method = "cv")
modelFit <- train(type~., data = training, method ="glm", trControl=control)
prediction <- predict(modelFit, newdata = testing)
confusionMatrix(prediction, testing$type)
```
Usefull to set a seed.

```{r setseedresult}
set.seed(1235)
modelFit3 <- train(type~., data=training, method="glm")
modelFit3
```

# plotting predicitons

Wage data
```{r wages}
library(ISLR)
require(ggplot2)
require(caret)
require(GGally)
require(dplyr)
data(Wage)
summary(Wage)

pairs(Wage)
Wage %>% select(age, race, wage, education) %>% ggpairs(., lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1)))

#set adide testing set
intrain <- createDataPartition(y=Wage$wage, p=0.7, list = FALSE)
training <- Wage[intrain,]
testin <- Wage[-intrain,]
dim(training)
dim(testin)
```

## Feature plot ::caret
```{r plots}
featurePlot(x = training[,c("age","race", "education")], y = training$wage, plot = "pairs")

qplot(age, wage, colour = education,data = training)+geom_smooth(method = 'lm', formula = y~x)
```

```{r cut}
require(Hmisc)
require(gridExtra)
cutwage <- cut2(training$wage, g=3)
table(cutwage)

cbind.data.frame(cutwage,training %>% select(race)) %>% ggpairs(., lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1)))
cbind.data.frame(cutwage,training %>% select(race)) %>% table() %>% prop.table(.,2)

qplot(cutwage, age, data = training, fill = cutwage, geom = c("violin"))

qplot(cutwage, age, data = training, fill = cutwage, geom = c("boxplot"))

p1 <- qplot(race, wage, data = training, fill = education, geom = c("boxplot"))


p1
p2 <-
qplot(
race,
wage,
data = training,
fill = education, color = education,
geom = c("boxplot", "jitter")
)
grid.arrange(p1, p2, ncol = 2)
```

### tables

```{r tables}
table(cutwage,training$jobclass) %>% prop.table(.,1)
```

### density plot

```{r density}
qplot(wage, colour= education, data = training, geom = "density")

qplot(wage, colour= race, data = training, geom = "density")
```

#### Notes 

* Make plots on the training set
    * dont explore testing
* look for
    * imbalances in outcome/predictors
    * otliers
    * groups of points not explained by a predictor
    * skewed variables

# preprocessing

needed for model -bases approaches.
## why preprocess?

```{r preprocess}
require(caret)
require(kernlab)
data("spam")
intrain <- createDataPartition(y=spam$type, p=0.75, list = FALSE)
training <- spam[intrain,]
testing <- spam[-intrain,]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")

qplot(type, log10(training$capitalAve), data = training, geom = "violin")

mean(training$capitalAve)
sd(training$capitalAve)
```

## standardize
```{r}
traincapave <- training$capitalAve
traincapaves <- (traincapave-mean(traincapave))/sd(traincapave)

summary(traincapaves)
sd(traincapaves)
```

standardisation will be done by the training set.

```{r}
testcapave <- testing$capitalAve
testcapaves <- (testcapave-mean(traincapave))/sd(traincapave)

summary(testcapaves)
sd(testcapaves)
```

### caret has preporcessing
this is standardization - without writing formulas

```{r caretpreporcess}
preObj <- preProcess(training[,-58], method = c("center","scale"))
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
```

can use this to apply to the testing
```{r}
testcapaves <- predict(preObj,testing[,-58])$capitalAve
mean(testcapaves)
sd(testcapaves)
```

can pass preProcess commands to train
```{r}
modelFit <- train(type~., data = training, preProcess = c("center","scale"), method = "glm")
modelFit
```

### Other transformations
```{r boxcox}
preObj <- preProcess(training[-58], method = c("BoxCox"))
trainCapAveS <- predict(preObj,training[-58])$capitalAve
par(mfrow = c(1,2))
hist(trainCapAveS)
qqnorm(trainCapAveS)

par(mfrow = c(1,2))
hist(training$capitalAve)
qqnorm(training$capitalAve)
```

### imputing data - helping with missing data

find neerest k rows with missing data - and averages them 


```{r imputingdata}
set.seed(42)

# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA

# Impute and standardize
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve

# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)


quantile(capAve- capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
```

* Trainign and testing must be processed in the same way
* test trainsformation will likely be imperfect
    * Especially if data sets collected at different times
* Be carefull with factor variables
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)

# covariate creating

1. from raw data to covariate (email to letter counts)
    * depends heavy on application
    * balancing act is summarization vs information loss
    * examples
        * text files - frequence of words, frequence of phrased, google ngrams, frequency of capitals
        * images - edges, cornergs, blobs, ridges
        * webpages - numbers and types of images, position of elements, colors, videos - A/B testing
        * people - height, wight, hair color, sex, country of origin
    * need more knowledge
    * better err on the side of more features
    * can be automated but use caution
2. transforming tidy covariates
    * features already created
    * more necessary for metthonds (regression, svms) that others (classification trees)
    * should be done only _on the training set_
    * best approach is through exploratory analysis (plotting/tables)
    * new covariates should be added to data frames
```{r}
require(kernlab)
data("spam")
spam$capitalAvesq <- spam$capitalAve^2
par(mfrow = c(1,2))
hist(spam$capitalAvesq)
qqnorm(spam$capitalAvesq)
```

```{r}
library(ISLR)
library(caret)
data("Wage")
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list = F)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
```
Commong covarites to add, dummy variables
convert factor variables to indicator variables

```{r}
table(training$jobclass)

dummies <- dummyVars(wage~jobclass, data = training)
head(predict(dummies, newdata=training))
```

Removing zero covariates
```{r}
nsv <- nearZeroVar(training, saveMetrics = T)
nsv
```

spline basis
```{r}
require(splines)
bsBasis <- bs(training$age, df=3)
head(bsBasis)
```
helps with curvy model fitting

## fitting curves with splines
fits 3 order polynomial
```{r}
lm1 <- lm(wage~bsBasis, data = training)
plot(training$age, training$wage, pch=19, cex=0.5)
points(training$age, predict(lm1, newdata=training), col="red", pch=19, cex=0.5)
```

## splines on the test set
```{r}
head(predict(bsBasis, age = testing$age))
```

## Notes and further reading

* Level 1 feature creation (raw data to covariates)
  * Science is key. Google "feature extraction for [data type]"
  * Err on overcreation of features
  * In some applications (images, voices) automated feature creation is possible/necessary
    * http://www.cs.nyu.edu/~yann/talks/lecun-ranzato-icml2013.pdf
* Level 2 feature creation (covariates to new covariates)
  * The function _preProcess_ in _caret_ will handle some preprocessing.
  * Create new covariates if you think they will improve fit
  * Use exploratory analysis on the training set for creating them
  * Be careful about overfitting!
* [preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)
* If you want to fit spline models, use the _gam_ method in the _caret_ package which allows smoothing of multiple variables.
* More on feature creation/data tidying in the Obtaining Data course from the Data Science course track. 

# principal componen analysys
```{r}
require(caret)
require(kernlab)
data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M>0.8, arr.ind = T)

names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
```

Both can be not useful
* wee might not need every
* a weighted combination of predictors might be better
* pick combinations for "most information" possible
* benefits
  * reduce predictor number
  * reduce noise

we could rotate the plot

Related problems.

* find a new set of multivariate variables that are uncorrelated and explain as much variance as possible - statistical goal
* if you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explans the original data. - data compression goal

