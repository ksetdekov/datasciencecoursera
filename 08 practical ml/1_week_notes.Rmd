---
title: "Practical machine learning  lectures 1"
author: "Kirill Setdekov"
date: "September 12 2019"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	cache = TRUE
)
```

# week 1

Elements of statistical learning.
https://web.stanford.edu/~hastie/ElemStatLearn/

caret package
```{r}
require(caret)
```

Machine learning by stanford on coursera

# what is prediction

1. probability/sampling from the pool of data
2. get training set and process it
3. build a prediction function

## process

* question  (formultate this)
* input data (find it and know its limitation)
* features (frquency of words)
* algorithm 
* parameters
* evaluation

## relative importance of steps

somitmes - we need to give up
garbage in - garbage out.
often mode data - better model

*good features*

- lead to data compression
- retain relevant info
- are created based on expert application knowledge

*mistakes*

- trying to automate feature selection
- not pay attention to data-specific quirks

algorithm sometimes do not matter that much

issues to sonsider
- interpetable 
- simple
- accurate
- fast
- scalable

# in sample error

*in sample* - error rate you get for a predictor in the training sample
*out of sample error* - on a testing sample

key idea

1. out of sample is what we want
2. sample error < out of sample error
3. reason - overfitting

```{r insampleout}
library(kernlab)
data("spam")
smallspam <- spam[sample(dim(spam)[1],size = 2000),]
smaplabel <- (smallspam$type=="spam")*1+1
plot(smallspam$capitalAve,col=smaplabel)
library(ggplot2)
ggplot(spam, aes(y = capitalAve, x = type, fill = type)) + geom_violin() +
labs(title = "Violin plot of capitalAve")+theme_bw()+ylim(0,20)
```
Can make a a prediction based on capitalAve
CapitalAve>2.4 "spam"
```{r ruleex}
rule2 <- function(x){
    prediction <- rep(NA, length(x))
    prediction[x>2.8] <- "spam"
    prediction[x<=2.8] <- "nonspam"
    return(prediction)
}
table(rule2(smallspam$capitalAve),smallspam$type)
table(rule2(spam$capitalAve),spam$type)
sum(rule2(spam$capitalAve)==spam$type)
library(party)
 cfit <- ctree(type~capitalAve, smallspam)
table(predict(cfit,spam),spam$type)
sum(predict(cfit,spam)==spam$type)/dim(spam)[1]

 cfit2 <- ctree(type~capitalAve+you, smallspam)
table(predict(cfit2,spam),spam$type)
sum(predict(cfit2,spam)==spam$type)/dim(spam)[1]

 cfit3 <- ctree(type~., smallspam)
table(predict(cfit3,spam),spam$type)
sum(predict(cfit3,spam)==spam$type)/dim(spam)[1]
```

# prediction study design

1. define error rate
2. split data into
    training
    testing
    validation(optional)
3. pick features (using cross validation)
4. on the training set pick prediction
use cross validation
5 if no validaiton
*apply 1 to test set*
6 if validaiton set
apply to test set and refine
*appy 1 time to validation set*

## avoid small sample size
###good rules
* big data

60% training
20% test
20% validation

* medium

60% training
40% testing

* if small
do cross validation
report this with caveat.

## remember
* set a validation set aside and dont look at it
* in genereal - random sample
* data must reflect structure of the problem
    * if evolve over time - split into random time chunks
* all subsets should reflect as much diversity as possible
    * random assignment does this
    * can also try to balance by features - but tricky

# types of errors
medical test e.g.
* *true positive* - correctyly identified (sick and diagnosed)
* *false positive* - incorrectyly identified (healthy and diagnosed)
* *false negative* - incorrectyly rejected (sick adn not identified)

$ Sensitivity = \frac{Positive test}{disease}$
$ Specificity = \frac{Negative test}{no desease}$
$ positivepredictive value = \frac{desease}{Positive test}$
$ Negative predictive value = \frac{no desease}{Negative test}$
$ accuracy =\frac{TP+TN}{all}$

## for continuous data
Mean square error (MSE)

$\frac{1}{n}\sum^n_{i=1}{(Prediction_i-Truth_i)^2}$

RMSE

$\sqrt{\frac{1}{n}\sum^n_{i=1}{(Prediction_i-Truth_i)^2}}$

## common error measures

1. mse (continious, sensitive to outliers)
2. mean abs deviation (continuous data, often more robust)
3. sensitivity (recall) (if you want few missed positives)
4. specificity (if you want fiew negatives called positives)
5. accuracy (waihts false positives and neagatives equally)
6. condocdance (one example is kappa)

# ROC curve

Receiver operating characteristic.
Sensitivity - Y axis, X - (1-specificity). Each point - is a given cutoff.

AUC = 0.5 - random guessing
AUC = 1: perfect classifier.
0.8 - good AUC.

```{r rocr}
# plot ROC
library(ROCR)
# cfit3,spam
predProb = sapply(predict(cfit3, newdata=spam,type="prob"),'[[',2)  # obtain probability of class 1 (second element from the lists)

roc_pred <- prediction(predProb, spam$type)
perf <- performance(roc_pred, "tpr", "fpr")
plot(perf, col="red")
abline(0,1,col="grey")
performance(roc_pred,"auc")@y.values
cutoff <- performance(roc_pred,"prbe")@x.values
cutoff2 <- (unlist(performance(roc_pred,"f")@x.values))[which.max(unlist(performance(roc_pred,"f")@y.values))]

custpred <- factor(ifelse(predProb > cutoff, "spam", "nonspam"))
table(spam$type, custpred)


caret::confusionMatrix(data = custpred, 
                  reference = spam$type, positive = "spam")


custpred <- factor(ifelse(predProb > cutoff2, "spam", "nonspam"))
table(spam$type, custpred)


caret::confusionMatrix(data = custpred, 
                  reference = spam$type, positive = "spam")
```

# cross validation
### approach

1. use training set
2. split into train/test
3. build model on train
4. evaluate on test.
5. repeat splitting and 2-4 and average estimated errors.

or K-fold cross validation- get a testing in 1/3, repeat 3 times, or 1/k and k times.

or. leave one out.
leave sample - predict 

## considerations

* for time seris - need chunks
* for k-fold
    * large k = less bias, more variance
    * small k = more bias, less variance
* sampling must be done without replacement
* random sampling with replacement - botstrapping.
    * underestimates the error.
    * can be corrected but hard.
* if cross validate to pick the model - need to estimate errors on indipendent data.


